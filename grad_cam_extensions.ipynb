{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":105730,"status":"ok","timestamp":1747930688447,"user":{"displayName":"Mohamad Sandakli","userId":"16501406067787434423"},"user_tz":-180},"id":"rry1CB8XTd7O","outputId":"404d3b94-71b7-4157-a66e-6f8e8b82851f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting segmentation-models-pytorch\n","  Downloading segmentation_models_pytorch-0.5.0-py3-none-any.whl.metadata (17 kB)\n","Requirement already satisfied: huggingface-hub>=0.24 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (0.31.2)\n","Requirement already satisfied: numpy>=1.19.3 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (2.0.2)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (11.2.1)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (0.5.3)\n","Requirement already satisfied: timm>=0.9 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (1.0.15)\n","Requirement already satisfied: torch>=1.8 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (2.6.0+cu124)\n","Requirement already satisfied: torchvision>=0.9 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (0.21.0+cu124)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (4.67.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (3.18.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (2025.3.2)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (6.0.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (2.32.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (4.13.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (3.1.6)\n","Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.8->segmentation-models-pytorch)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.8->segmentation-models-pytorch)\n","  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.8->segmentation-models-pytorch)\n","  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.8->segmentation-models-pytorch)\n","  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.8->segmentation-models-pytorch)\n","  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.8->segmentation-models-pytorch)\n","  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.8->segmentation-models-pytorch)\n","  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.8->segmentation-models-pytorch)\n","  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.8->segmentation-models-pytorch)\n","  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.4.127)\n","Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.8->segmentation-models-pytorch)\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8->segmentation-models-pytorch) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8->segmentation-models-pytorch) (3.0.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (2025.4.26)\n","Downloading segmentation_models_pytorch-0.5.0-py3-none-any.whl (154 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.8/154.8 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m67.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m45.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m85.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, segmentation-models-pytorch\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n","    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.6.82\n","    Uninstalling nvidia-curand-cu12-10.3.6.82:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n","    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n","      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n","    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n","    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n","    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n","      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n","    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n","    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n","Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 segmentation-models-pytorch-0.5.0\n"]}],"source":["!pip install segmentation-models-pytorch"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":5529,"status":"ok","timestamp":1747930693995,"user":{"displayName":"Mohamad Sandakli","userId":"16501406067787434423"},"user_tz":-180},"id":"t5na4j_lTgac","outputId":"a8c60171-7170-4a0f-dbf0-112e03b517bc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting rasterio\n","  Downloading rasterio-1.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.1 kB)\n","Collecting affine (from rasterio)\n","  Downloading affine-2.4.0-py3-none-any.whl.metadata (4.0 kB)\n","Requirement already satisfied: attrs in /usr/local/lib/python3.11/dist-packages (from rasterio) (25.3.0)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from rasterio) (2025.4.26)\n","Requirement already satisfied: click>=4.0 in /usr/local/lib/python3.11/dist-packages (from rasterio) (8.2.0)\n","Collecting cligj>=0.5 (from rasterio)\n","  Downloading cligj-0.7.2-py3-none-any.whl.metadata (5.0 kB)\n","Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.11/dist-packages (from rasterio) (2.0.2)\n","Collecting click-plugins (from rasterio)\n","  Downloading click_plugins-1.1.1-py2.py3-none-any.whl.metadata (6.4 kB)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from rasterio) (3.2.3)\n","Downloading rasterio-1.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (22.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.2/22.2 MB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading cligj-0.7.2-py3-none-any.whl (7.1 kB)\n","Downloading affine-2.4.0-py3-none-any.whl (15 kB)\n","Downloading click_plugins-1.1.1-py2.py3-none-any.whl (7.5 kB)\n","Installing collected packages: cligj, click-plugins, affine, rasterio\n","Successfully installed affine-2.4.0 click-plugins-1.1.1 cligj-0.7.2 rasterio-1.4.3\n"]}],"source":["!pip install rasterio"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":3410,"status":"ok","timestamp":1747930697419,"user":{"displayName":"Mohamad Sandakli","userId":"16501406067787434423"},"user_tz":-180},"id":"p8hghKL3Ti5x","outputId":"248d4998-a644-4116-d300-4cfd0bfe77a3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting ttach\n","  Downloading ttach-0.0.3-py3-none-any.whl.metadata (5.2 kB)\n","Downloading ttach-0.0.3-py3-none-any.whl (9.8 kB)\n","Installing collected packages: ttach\n","Successfully installed ttach-0.0.3\n"]}],"source":["!pip install ttach"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":10776,"status":"ok","timestamp":1747930708211,"user":{"displayName":"Mohamad Sandakli","userId":"16501406067787434423"},"user_tz":-180},"id":"To7dzc1ITld0","outputId":"8a36bc65-a027-4038-f160-cbb66dcf8107"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting grad-cam\n","  Downloading grad-cam-1.5.5.tar.gz (7.8 MB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/7.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.2/7.8 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/7.8 MB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m5.9/7.8 MB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m66.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m51.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from grad-cam) (2.0.2)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from grad-cam) (11.2.1)\n","Requirement already satisfied: torch>=1.7.1 in /usr/local/lib/python3.11/dist-packages (from grad-cam) (2.6.0+cu124)\n","Requirement already satisfied: torchvision>=0.8.2 in /usr/local/lib/python3.11/dist-packages (from grad-cam) (0.21.0+cu124)\n","Requirement already satisfied: ttach in /usr/local/lib/python3.11/dist-packages (from grad-cam) (0.0.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from grad-cam) (4.67.1)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (from grad-cam) (4.11.0.86)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from grad-cam) (3.10.0)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from grad-cam) (1.6.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->grad-cam) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->grad-cam) (4.13.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->grad-cam) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->grad-cam) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->grad-cam) (2025.3.2)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->grad-cam) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->grad-cam) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->grad-cam) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->grad-cam) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->grad-cam) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->grad-cam) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->grad-cam) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->grad-cam) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->grad-cam) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->grad-cam) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->grad-cam) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->grad-cam) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->grad-cam) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->grad-cam) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->grad-cam) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.7.1->grad-cam) (1.3.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->grad-cam) (1.3.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->grad-cam) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->grad-cam) (4.58.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->grad-cam) (1.4.8)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->grad-cam) (24.2)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->grad-cam) (3.2.3)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->grad-cam) (2.9.0.post0)\n","Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->grad-cam) (1.15.3)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->grad-cam) (1.5.0)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->grad-cam) (3.6.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->grad-cam) (1.17.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.7.1->grad-cam) (3.0.2)\n","Building wheels for collected packages: grad-cam\n","  Building wheel for grad-cam (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for grad-cam: filename=grad_cam-1.5.5-py3-none-any.whl size=44283 sha256=79277a029fafc4b9e7f603b65a4f4d73cb7926ecbb64634c2e7cf2047aab85f1\n","  Stored in directory: /root/.cache/pip/wheels/bc/52/78/893c3b94279ef238f43a9e89608af648de401b96415bebbd1f\n","Successfully built grad-cam\n","Installing collected packages: grad-cam\n","Successfully installed grad-cam-1.5.5\n"]}],"source":["!pip install grad-cam"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":15572,"status":"ok","timestamp":1747930723796,"user":{"displayName":"Mohamad Sandakli","userId":"16501406067787434423"},"user_tz":-180},"id":"LISfbSUZTpZl"},"outputs":[],"source":["from pytorch_grad_cam import GradCAM, ScoreCAM, GradCAMPlusPlus, AblationCAM, XGradCAM, EigenCAM, FullGrad\n","from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n","from pytorch_grad_cam.utils.image import show_cam_on_image, preprocess_image,deprocess_image\n","from pytorch_grad_cam.utils.find_layers import find_layer_predicate_recursive"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":77,"status":"ok","timestamp":1747930723809,"user":{"displayName":"Mohamad Sandakli","userId":"16501406067787434423"},"user_tz":-180},"id":"cQnzV1FZTtpn"},"outputs":[],"source":["import pandas as pd\n","from tabulate import tabulate"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":7053,"status":"ok","timestamp":1747930730871,"user":{"displayName":"Mohamad Sandakli","userId":"16501406067787434423"},"user_tz":-180},"id":"XEhTVd2IsqWN"},"outputs":[],"source":["# Import the required packages\n","\n","import torch\n","import argparse\n","import os\n","import cv2\n","\n","from segmentation_models_pytorch import Unet\n","\n","from skimage.io import imread,imsave\n","import numpy as np\n","import rasterio as rio\n","import geopandas as gpd\n","import matplotlib.pyplot as plt\n","import imageio\n","\n","from utils import *\n","from color_map import cm_data\n","from rasterio.features import shapes\n","from tqdm.notebook import tqdm"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":316,"status":"ok","timestamp":1747930731216,"user":{"displayName":"Mohamad Sandakli","userId":"16501406067787434423"},"user_tz":-180},"id":"_WO8qJq3sqWU"},"outputs":[],"source":["# load your model with pre-trained model weights\n","\n","MODEL_PATH = './pretrained_model'\n","THRESH = 0.5\n","ALPHA = 0.85\n","SCALE = None\n","\n","\n","model = Unet(\n","        encoder_name = \"tu-tf_efficientnet_b0\",\n","        encoder_depth= 5,\n","        encoder_weights = None,\n","        decoder_use_batchnorm = True,\n","        decoder_channels = (256, 128, 64, 32, 16),\n","        decoder_attention_type = None,\n","        in_channels= 3,\n","        classes = 3,\n","        activation = 'sigmoid',\n","        aux_params = None,\n","    )\n","\n","model = load_model(model,MODEL_PATH)\n","# model.cuda()"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":172,"status":"ok","timestamp":1747930731403,"user":{"displayName":"Mohamad Sandakli","userId":"16501406067787434423"},"user_tz":-180},"id":"qerUn3gXUrzU"},"outputs":[],"source":["# Define the CAM-based Extensions classes\n","from PIL import Image\n","import numpy as np\n","import ttach as tta\n","import sys\n","import torch\n","import warnings\n","from skimage.io import imread\n","import matplotlib.pyplot as plt\n","from segmentation_models_pytorch import Unet\n","import argparse\n","import os\n","from typing import Callable, List\n","import cv2\n","import tqdm\n","from pytorch_grad_cam.utils.image import show_cam_on_image, preprocess_image\n","from skimage.segmentation import watershed\n","from skimage.measure import label\n","from pytorch_grad_cam.utils.svd_on_activations import get_2d_projection\n","from pytorch_grad_cam.utils.image import scale_cam_image\n","from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n","from pytorch_grad_cam.utils.find_layers import replace_layer_recursive\n","\n","warnings.filterwarnings('ignore')\n","warnings.simplefilter('ignore')\n","\n","\n","# Grad-Cam Classes\n","class ActivationsAndGradients:\n","    \"\"\" Class for extracting activations and\n","    registering gradients from targetted intermediate layers \"\"\"\n","\n","    def __init__(self, model, target_layers, reshape_transform):\n","        self.model = model\n","        self.gradients = []\n","        self.activations = []\n","        self.reshape_transform = reshape_transform\n","        self.handles = []\n","        for target_layer in target_layers:\n","            self.handles.append(\n","                target_layer.register_forward_hook(self.save_activation))\n","            # Because of https://github.com/pytorch/pytorch/issues/61519,\n","            # we don't use backward hook to record gradients.\n","            self.handles.append(\n","                target_layer.register_forward_hook(self.save_gradient))\n","\n","    def save_activation(self, module, input, output):\n","        activation = output\n","        if self.reshape_transform is not None:\n","            activation = self.reshape_transform(activation)\n","        self.activations.append(activation.cpu().detach())\n","\n","    def save_gradient(self, module, input, output):\n","        if not hasattr(output, \"requires_grad\") or not output.requires_grad:\n","            # You can only register hooks on tensor requires grad.\n","            return\n","\n","        # Gradients are computed in reverse order\n","        def _store_grad(grad):\n","            if self.reshape_transform is not None:\n","                grad = self.reshape_transform(grad)\n","            self.gradients = [grad.cpu().detach()] + self.gradients\n","            # self.gradients = [torch.mul(t, -1) for t in self.gradients]\n","\n","        output.register_hook(_store_grad)\n","\n","    def __call__(self, x):\n","        self.gradients = []\n","        self.activations = []\n","        return self.model(x)\n","\n","    def release(self):\n","        for handle in self.handles:\n","            handle.remove()\n","\n","\n","\n","\n","class AblationLayer(torch.nn.Module):\n","    def __init__(self):\n","        super(AblationLayer, self).__init__()\n","\n","    def objectiveness_mask_from_svd(self, activations, threshold=0.01):\n","        \"\"\" Experimental method to get a binary mask to compare if the activation is worth ablating.\n","            The idea is to apply the EigenCAM method by doing PCA on the activations.\n","            Then we create a binary mask by comparing to a low threshold.\n","            Areas that are masked out, are probably not interesting anyway.\n","        \"\"\"\n","\n","        projection = get_2d_projection(activations[None, :])[0, :]\n","        projection = np.abs(projection)\n","        projection = projection - projection.min()\n","        projection = projection / projection.max()\n","        projection = projection > threshold\n","        return projection\n","\n","    def activations_to_be_ablated(\n","            self,\n","            activations,\n","            ratio_channels_to_ablate=1.0):\n","        \"\"\" Experimental method to get a binary mask to compare if the activation is worth ablating.\n","            Create a binary CAM mask with objectiveness_mask_from_svd.\n","            Score each Activation channel, by seeing how much of its values are inside the mask.\n","            Then keep the top channels.\n","\n","        \"\"\"\n","        if ratio_channels_to_ablate == 1.0:\n","            self.indices = np.int32(range(activations.shape[0]))\n","            return self.indices\n","\n","        projection = self.objectiveness_mask_from_svd(activations)\n","\n","        scores = []\n","        for channel in activations:\n","            normalized = np.abs(channel)\n","            normalized = normalized - normalized.min()\n","            normalized = normalized / np.max(normalized)\n","            score = (projection * normalized).sum() / normalized.sum()\n","            scores.append(score)\n","        scores = np.float32(scores)\n","\n","        indices = list(np.argsort(scores))\n","        high_score_indices = indices[::-\n","                                     1][: int(len(indices) *\n","                                              ratio_channels_to_ablate)]\n","        low_score_indices = indices[: int(\n","            len(indices) * ratio_channels_to_ablate)]\n","        self.indices = np.int32(high_score_indices + low_score_indices)\n","        return self.indices\n","\n","    def set_next_batch(\n","            self,\n","            input_batch_index,\n","            activations,\n","            num_channels_to_ablate):\n","        \"\"\" This creates the next batch of activations from the layer.\n","            Just take corresponding batch member from activations, and repeat it num_channels_to_ablate times.\n","        \"\"\"\n","        self.activations = activations[input_batch_index, :, :, :].clone(\n","        ).unsqueeze(0).repeat(num_channels_to_ablate, 1, 1, 1)\n","\n","    def __call__(self, x, test=None):\n","        output = self.activations\n","        for i in range(output.size(0)):\n","            # Commonly the minimum activation will be 0,\n","            # And then it makes sense to zero it out.\n","            # However depending on the architecture,\n","            # If the values can be negative, we use very negative values\n","            # to perform the ablation, deviating from the paper.\n","            if torch.min(output) == 0:\n","                output[i, self.indices[i], :] = 0\n","            else:\n","                ABLATION_VALUE = 1e7\n","                output[i, self.indices[i], :] = torch.min(\n","                    output) - ABLATION_VALUE\n","\n","        return output\n","\n","\n","\n","\n","class GRADCAM_Extensions:\n","    def __init__(self, extension, model: torch.nn.Module, target_layers: List[torch.nn.Module], use_cuda: bool = False,\n","                 reshape_transform: Callable = None, compute_input_gradient: bool = False,\n","                 uses_gradients: bool = True) -> None:\n","        self.model = model.eval()\n","        self.target_layers = target_layers\n","        self.cuda = use_cuda\n","        if self.cuda:\n","            self.model = model.cuda()\n","        self.reshape_transform = reshape_transform\n","        self.compute_input_gradient = compute_input_gradient\n","        self.uses_gradients = uses_gradients\n","        self.activations_and_grads = ActivationsAndGradients(self.model, target_layers, reshape_transform)\n","        self.extension = extension\n","\n","    def forward(self, input_tensor: torch.Tensor, targets: List[torch.nn.Module],\n","                eigen_smooth: bool = False) -> np.ndarray:\n","\n","        # print('\\n----------------------------------------Seg-Grad-Cam----------------------------------------------------\\n')\n","\n","        if self.cuda:\n","            input_tensor = input_tensor.cuda()\n","        if self.compute_input_gradient:\n","            input_tensor = torch.autograd.Variable(input_tensor, requires_grad=True)\n","\n","        outputs = self.activations_and_grads(input_tensor)\n","        if targets is None:\n","            target_categories = np.argmax(outputs.cpu().data.numpy(), axis=-1)\n","            targets = [ClassifierOutputTarget(category) for category in target_categories]\n","\n","        if self.uses_gradients:\n","            self.model.zero_grad()\n","            loss = sum([target(output) for target, output in zip(targets, outputs)])\n","            loss.backward(retain_graph=True)\n","\n","        activations_list = [a.cpu().data.numpy() for a in self.activations_and_grads.activations]\n","        grads_list = [g.cpu().data.numpy() for g in self.activations_and_grads.gradients]\n","        target_size = input_tensor.size(-1), input_tensor.size(-2)\n","        # print('Activations list size: ', len(activations_list))\n","        # print('Gradients list size: ', len(grads_list))\n","        # print('Target layer size: ', len(self.target_layers))\n","\n","        cam_per_target_layer = []\n","        # Loop over the saliency image from every layer\n","        for i in range(len(self.target_layers)):\n","            target_layer = self.target_layers[i]\n","            # print('\\t\\t\\t-----------------------\\n')\n","            # print('Target Layer ', i + 1, ': ', target_layer)\n","            layer_activations = None\n","            layer_grads = None\n","            if i < len(activations_list):\n","                layer_activations = activations_list[i]\n","            if i < len(grads_list):\n","                layer_grads = grads_list[i]\n","\n","\n","            if  self.extension == \"grad_cam\":\n","                weights = np.mean(layer_grads, axis=(2, 3))\n","                weighs_up = weights[:, :, None, None]\n","                weighted_activations = weights[:, :, None, None] * layer_activations\n","                if eigen_smooth:\n","                    cam = get_2d_projection(weighted_activations)\n","                    print(\"Cam image per layer size: \", cam.shape)\n","                else:\n","                    cam = weighted_activations.sum(axis=1)\n","\n","            elif self.extension == \"hires_cam\":\n","                elementwise_activations = layer_grads * layer_activations\n","                if eigen_smooth:\n","                    cam = get_2d_projection(elementwise_activations)\n","                else:\n","                    cam = elementwise_activations.sum(axis=1)\n","\n","            elif self.extension == \"ew_cam\":\n","                elementwise_activations = np.maximum(layer_grads * layer_activations, 0)\n","                if eigen_smooth:\n","                    cam = get_2d_projection(elementwise_activations)\n","                else:\n","                    cam = elementwise_activations.sum(axis=1)\n","\n","            elif self.extension == \"grad_cam_pp\":\n","                grads_power_2 = layer_grads**2\n","                grads_power_3 = grads_power_2 * layer_grads\n","                # Equation 19 in https://arxiv.org/abs/1710.11063\n","                sum_activations = np.sum(layer_activations, axis=(2, 3))\n","                eps = 0.000001\n","                aij = grads_power_2 / (2 * grads_power_2 + sum_activations[:, :, None, None] * grads_power_3 + eps)\n","                # Now bring back the ReLU from eq.7 in the paper,\n","                # And zero out aijs where the activations are 0\n","                aij = np.where(layer_grads != 0, aij, 0)\n","                weights = np.maximum(layer_grads, 0) * aij\n","                weights = np.sum(weights, axis=(2, 3))\n","                weighted_activations = weights[:, :, None, None] * layer_activations\n","                if eigen_smooth:\n","                    cam = get_2d_projection(weighted_activations)\n","                    print(\"Cam image per layer size: \", cam.shape)\n","                else:\n","                    cam = weighted_activations.sum(axis=1)\n","\n","\n","\n","            elif self.extension == \"x_grad_cam\":\n","                sum_activations = np.sum(layer_activations, axis=(2, 3))\n","                eps = 1e-7\n","                weights = layer_grads * layer_activations / \\\n","                (sum_activations[:, :, None, None] + eps)\n","                weights = weights.sum(axis=(2, 3))\n","                weighted_activations = weights[:, :, None, None] * layer_activations\n","                if eigen_smooth:\n","                    cam = get_2d_projection(weighted_activations)\n","                    print(\"Cam image per layer size: \", cam.shape)\n","                else:\n","                    cam = weighted_activations.sum(axis=1)\n","\n","\n","            elif self.extension == \"score_cam\":\n","                with torch.no_grad():\n","                    upsample = torch.nn.UpsamplingBilinear2d(size=input_tensor.shape[-2:])\n","                    activation_tensor = torch.from_numpy(layer_activations)\n","                    if self.cuda:\n","                        activation_tensor = activation_tensor.cuda()\n","                    upsampled = upsample(activation_tensor)\n","                    maxs = upsampled.view(upsampled.size(0), upsampled.size(1), -1).max(dim=-1)[0]\n","                    mins = upsampled.view(upsampled.size(0), upsampled.size(1), -1).min(dim=-1)[0]\n","                    maxs, mins = maxs[:, :, None, None], mins[:, :, None, None]\n","                    upsampled = (upsampled - mins) / (maxs - mins)\n","\n","                    input_tensors = input_tensor[:, None, :, :] * upsampled[:, :, None, :, :]\n","                    if hasattr(self, \"batch_size\"):\n","                        BATCH_SIZE = self.batch_size\n","                    else:\n","                        BATCH_SIZE = 8\n","\n","\n","                    scores = []\n","                    for target, tensor in zip(targets, input_tensors):\n","                        for i in tqdm.tqdm(range(0, tensor.size(0), BATCH_SIZE)):\n","                            batch = tensor[i: i + BATCH_SIZE, :]\n","                            outputs = [target(o).cpu().item() for o in self.model(batch)]\n","                            scores.extend(outputs)\n","                    scores = torch.Tensor(scores)\n","                    scores = scores.view(layer_activations.shape[0], layer_activations.shape[1])\n","                    weights = torch.nn.Softmax(dim=-1)(scores).numpy()\n","\n","                weighted_activations = weights[:, :, None, None] * layer_activations\n","                if eigen_smooth:\n","                    cam = get_2d_projection(weighted_activations)\n","                    print(\"Cam image per layer size: \", cam.shape)\n","                else:\n","                    cam = weighted_activations.sum(axis=1)\n","\n","            elif self.extension == \"layer_cam\":\n","                spatial_weighted_activations = np.maximum(layer_grads, 0) * layer_activations\n","                if eigen_smooth:\n","                    cam = get_2d_projection(spatial_weighted_activations)\n","                else:\n","                    cam = spatial_weighted_activations.sum(axis=1)\n","\n","\n","            elif self.extension == \"eigen_cam\":\n","                cam = get_2d_projection(layer_activations)\n","\n","            elif self.extension == \"eigen_grad_cam\":\n","                cam = get_2d_projection(layer_grads * layer_activations)\n","\n","            else:\n","                print(\"Unkown Extension. Please use one of the following: grad_cam - hires_cam - ew_cam -  grad_cam_pp - x_grad_cam - score_cam - layer_cam - eigen_cam -  eigen_grad_cam\")\n","\n","\n","\n","            cam = np.maximum(cam, 0)\n","            # print(\"Cam image  Max per layer size: \", cam.shape)\n","            scaled = scale_cam_image(cam, target_size)\n","            # print(\"Scaled Cam image per layer size: \", scaled.shape)\n","            cam_per_target_layer.append(scaled[:, None, :])\n","\n","        # print(\"Cam image list size: \", len(cam_per_target_layer))\n","        cam_per_target_layer = np.concatenate(cam_per_target_layer, axis=1)\n","        # print(\"Cam image list Concat size: \", len(cam_per_target_layer))\n","        cam_per_target_layer = np.maximum(cam_per_target_layer, 0)\n","        # print(\"Cam image list (max) size: \", len(cam_per_target_layer))\n","        result = np.mean(cam_per_target_layer, axis=1) # old: mean\n","        # print(\"+++ Averaged CAM list size: \", result.shape)\n","\n","\n","        return scale_cam_image(result) # result\n","\n","    def __call__(self, input_tensor: torch.Tensor, targets: List[torch.nn.Module] = None, aug_smooth: bool = False,\n","                 eigen_smooth: bool = False) -> np.ndarray:\n","        # Smooth the CAM result with test time augmentation\n","        if aug_smooth is True:\n","            transforms = tta.Compose(\n","                [\n","                    tta.HorizontalFlip(),\n","                    tta.Multiply(factors=[0.9, 1, 1.1]),\n","                ]\n","            )\n","            cams = []\n","            for transform in transforms:\n","                augmented_tensor = transform.augment_image(input_tensor)\n","                cam = self.forward(augmented_tensor, targets, eigen_smooth)\n","                # The ttach library expects a tensor of size BxCxHxW\n","                cam = cam[:, None, :, :]\n","                cam = torch.from_numpy(cam)\n","                cam = transform.deaugment_mask(cam)\n","                # Back to numpy float32, HxW\n","                cam = cam.numpy()\n","                cam = cam[:, 0, :, :]\n","                cams.append(cam)\n","            cam = np.mean(np.float32(cams), axis=0)\n","            return cam\n","        else:\n","            return self.forward(input_tensor, targets, eigen_smooth)\n","\n","    def __del__(self):\n","        self.activations_and_grads.release()\n","\n","    def __enter__(self):\n","        return self\n","\n","    def __exit__(self, exc_type, exc_value, exc_tb):\n","        self.activations_and_grads.release()\n","        if isinstance(exc_value, IndexError):\n","            # Handle IndexError here...\n","            print(\n","                f\"An exception occurred in CAM with block: {exc_type}. Message: {exc_value}\")\n","            return True\n","\n","\n","\n","\n","class GRADCAMEXTENDED_AblationCAM:\n","    def __init__(self, model: torch.nn.Module, target_layers: List[torch.nn.Module], use_cuda: bool = False,\n","                 reshape_transform: Callable = None, compute_input_gradient: bool = False,\n","                 uses_gradients: bool = True,\n","                 ablation_layer: torch.nn.Module = AblationLayer(),\n","                 batch_size: int = 32,\n","                 ratio_channels_to_ablate: float = 1.0) -> None:\n","        self.model = model.eval()\n","        self.target_layers = target_layers\n","        self.cuda = use_cuda\n","        if self.cuda:\n","            self.model = model.cuda()\n","        self.reshape_transform = reshape_transform\n","        self.compute_input_gradient = compute_input_gradient\n","        self.uses_gradients = uses_gradients\n","        self.activations_and_grads = ActivationsAndGradients(self.model, target_layers, reshape_transform)\n","        self.batch_size = batch_size\n","        self.ablation_layer = ablation_layer\n","        self.ratio_channels_to_ablate = ratio_channels_to_ablate\n","\n","\n","    def save_activation(self, module, input, output) -> None:\n","        \"\"\" Helper function to save the raw activations from the target layer \"\"\"\n","        self.activations = output\n","\n","    def assemble_ablation_scores(self,\n","                                 new_scores: list,\n","                                 original_score: float,\n","                                 ablated_channels: np.ndarray,\n","                                 number_of_channels: int) -> np.ndarray:\n","        \"\"\" Take the value from the channels that were ablated,\n","            and just set the original score for the channels that were skipped \"\"\"\n","\n","        index = 0\n","        result = []\n","        sorted_indices = np.argsort(ablated_channels)\n","        ablated_channels = ablated_channels[sorted_indices]\n","        new_scores = np.float32(new_scores)[sorted_indices]\n","\n","        for i in range(number_of_channels):\n","            if index < len(ablated_channels) and ablated_channels[index] == i:\n","                weight = new_scores[index]\n","                index = index + 1\n","            else:\n","                weight = original_score\n","            result.append(weight)\n","\n","        return result\n","\n","\n","    def forward(self, input_tensor: torch.Tensor, targets: List[torch.nn.Module],\n","                eigen_smooth: bool = False) -> np.ndarray:\n","\n","        # print('\\n----------------------------------------Seg-Grad-Cam----------------------------------------------------\\n')\n","\n","        if self.cuda:\n","            input_tensor = input_tensor.cuda()\n","        if self.compute_input_gradient:\n","            input_tensor = torch.autograd.Variable(input_tensor, requires_grad=True)\n","\n","        outputs = self.activations_and_grads(input_tensor)\n","        if targets is None:\n","            target_categories = np.argmax(outputs.cpu().data.numpy(), axis=-1)\n","            targets = [ClassifierOutputTarget(category) for category in target_categories]\n","\n","        if self.uses_gradients:\n","            self.model.zero_grad()\n","            loss = sum([target(output) for target, output in zip(targets, outputs)])\n","            loss.backward(retain_graph=True)\n","\n","        activations_list = [a.cpu().data.numpy() for a in self.activations_and_grads.activations]\n","        grads_list = [g.cpu().data.numpy() for g in self.activations_and_grads.gradients]\n","        target_size = input_tensor.size(-1), input_tensor.size(-2)\n","        # print('Activations list size: ', len(activations_list))\n","        # print('Gradients list size: ', len(grads_list))\n","        # print('Target layer size: ', len(self.target_layers))\n","\n","        cam_per_target_layer = []\n","        # Loop over the saliency image from every layer\n","        for i in range(len(self.target_layers)):\n","            target_layer = self.target_layers[i]\n","            # print('\\t\\t\\t-----------------------\\n')\n","            # print('Target Layer ', i + 1, ': ', target_layer)\n","            layer_activations = None\n","            layer_grads = None\n","            if i < len(activations_list):\n","                layer_activations = activations_list[i]\n","            if i < len(grads_list):\n","                layer_grads = grads_list[i]\n","\n","\n","            # get weights\n","            # Do a forward pass, compute the target scores, and cache the\n","            # activations\n","            handle = target_layer.register_forward_hook(self.save_activation)\n","            with torch.no_grad():\n","                outputs = self.model(input_tensor)\n","                handle.remove()\n","                original_scores = np.float32(\n","                    [target(output).cpu().item() for target, output in zip(targets, outputs)])\n","\n","            # Replace the layer with the ablation layer.\n","            # When we finish, we will replace it back, so the original model is\n","            # unchanged.\n","            ablation_layer = self.ablation_layer\n","            replace_layer_recursive(self.model, target_layer, ablation_layer)\n","\n","            number_of_channels = layer_activations.shape[1]\n","            weights = []\n","            # This is a \"gradient free\" method, so we don't need gradients here.\n","            with torch.no_grad():\n","                # Loop over each of the batch images and ablate activations for it.\n","                for batch_index, (target, tensor) in enumerate(\n","                        zip(targets, input_tensor)):\n","                    new_scores = []\n","                    batch_tensor = tensor.repeat(self.batch_size, 1, 1, 1)\n","\n","                    # Check which channels should be ablated. Normally this will be all channels,\n","                    # But we can also try to speed this up by using a low\n","                    # ratio_channels_to_ablate.\n","                    channels_to_ablate = ablation_layer.activations_to_be_ablated(\n","                        layer_activations[batch_index, :], self.ratio_channels_to_ablate)\n","                    number_channels_to_ablate = len(channels_to_ablate)\n","\n","                    for i in tqdm.tqdm(range(0, number_channels_to_ablate, self.batch_size)):\n","                        if i + self.batch_size > number_channels_to_ablate:\n","                            batch_tensor = batch_tensor[:(number_channels_to_ablate - i)]\n","\n","                        # Change the state of the ablation layer so it ablates the next channels.\n","                        # TBD: Move this into the ablation layer forward pass.\n","                        ablation_layer.set_next_batch(input_batch_index=batch_index,activations=self.activations,\n","                            num_channels_to_ablate=batch_tensor.size(0))\n","\n","\n","                        score = [target(o).cpu().item() for o in self.model(batch_tensor)]\n","                        new_scores.extend(score)\n","\n","                        ablation_layer.indices = ablation_layer.indices[batch_tensor.size(0):]\n","\n","\n","                    new_scores = self.assemble_ablation_scores(new_scores,original_scores[batch_index], channels_to_ablate,\n","                        number_of_channels)\n","                    weights.extend(new_scores)\n","\n","            weights = np.float32(weights)\n","            weights = weights.reshape(layer_activations.shape[:2])\n","            original_scores = original_scores[:, None]\n","            weights = (original_scores - weights) / original_scores\n","\n","            # Replace the model back to the original state\n","            #-----------------------------------\n","            replace_layer_recursive(self.model, ablation_layer, target_layer)\n","\n","\n","            # Equation 3.1\n","            weighted_activations = weights[:, :, None, None] * layer_activations\n","\n","            # Equation 3.2\n","            if eigen_smooth:\n","                cam = get_2d_projection(weighted_activations)\n","            else:\n","                cam = weighted_activations.sum(axis=1)\n","\n","            cam = np.maximum(cam, 0)\n","            # print(\"Cam image  Max per layer size: \", cam.shape)\n","            scaled = scale_cam_image(cam, target_size)\n","            # print(\"Scaled Cam image per layer size: \", scaled.shape)\n","            cam_per_target_layer.append(scaled[:, None, :])\n","\n","        # print(\"Cam image list size: \", len(cam_per_target_layer))\n","        cam_per_target_layer = np.concatenate(cam_per_target_layer, axis=1)\n","        # print(\"Cam image list Concat size: \", len(cam_per_target_layer))\n","        cam_per_target_layer = np.maximum(cam_per_target_layer, 0)\n","        # print(\"Cam image list (max) size: \", len(cam_per_target_layer))\n","        result = np.mean(cam_per_target_layer, axis=1) # old: mean\n","        # print(\"+++ Averaged CAM list size: \", result.shape)\n","\n","\n","        return scale_cam_image(result) # result\n","\n","    def __call__(self, input_tensor: torch.Tensor, targets: List[torch.nn.Module] = None, aug_smooth: bool = False,\n","                 eigen_smooth: bool = False) -> np.ndarray:\n","        # Smooth the CAM result with test time augmentation\n","        if aug_smooth is True:\n","            transforms = tta.Compose(\n","                [\n","                    tta.HorizontalFlip(),\n","                    tta.Multiply(factors=[0.9, 1, 1.1]),\n","                ]\n","            )\n","            cams = []\n","            for transform in transforms:\n","                augmented_tensor = transform.augment_image(input_tensor)\n","                cam = self.forward(augmented_tensor, targets, eigen_smooth)\n","                # The ttach library expects a tensor of size BxCxHxW\n","                cam = cam[:, None, :, :]\n","                cam = torch.from_numpy(cam)\n","                cam = transform.deaugment_mask(cam)\n","                # Back to numpy float32, HxW\n","                cam = cam.numpy()\n","                cam = cam[:, 0, :, :]\n","                cams.append(cam)\n","            cam = np.mean(np.float32(cams), axis=0)\n","            return cam\n","        else:\n","            return self.forward(input_tensor, targets, eigen_smooth)\n","\n","    def __del__(self):\n","        self.activations_and_grads.release()\n","\n","    def __enter__(self):\n","        return self\n","\n","    def __exit__(self, exc_type, exc_value, exc_tb):\n","        self.activations_and_grads.release()\n","        if isinstance(exc_value, IndexError):\n","            # Handle IndexError here...\n","            print(\n","                f\"An exception occurred in CAM with block: {exc_type}. Message: {exc_value}\")\n","            return True\n","\n","\n","\n","\n","\n","from pytorch_grad_cam.utils.find_layers import find_layer_predicate_recursive\n","from pytorch_grad_cam.utils.image import scale_accross_batch_and_channels\n","\n","\n","class GRADCAMEXTENDED_FullGrad:\n","    def __init__(self, model: torch.nn.Module, target_layers: List[torch.nn.Module], use_cuda: bool = False,\n","                 reshape_transform: Callable = None, compute_input_gradient: bool = True,\n","                 uses_gradients: bool = True) -> None:\n","\n","        if len(target_layers) > 0:\n","            print(\n","                \"Warning: target_layers is ignored in FullGrad. All bias layers will be used instead\")\n","\n","        def layer_with_2D_bias(layer):\n","            bias_target_layers = [torch.nn.Conv2d, torch.nn.BatchNorm2d]\n","            if type(layer) in bias_target_layers and layer.bias is not None:\n","                return True\n","            return False\n","        target_layers = find_layer_predicate_recursive(model, layer_with_2D_bias)\n","        self.bias_data = [self.get_bias_data(layer).cpu().numpy() for layer in target_layers]\n","\n","\n","\n","\n","        self.model = model.eval()\n","        self.target_layers = target_layers\n","        self.cuda = use_cuda\n","        if self.cuda:\n","            self.model = model.cuda()\n","        self.reshape_transform = reshape_transform\n","        self.compute_input_gradient = compute_input_gradient\n","        self.uses_gradients = uses_gradients\n","        self.activations_and_grads = ActivationsAndGradients(self.model, target_layers, reshape_transform)\n","\n","\n","    def get_bias_data(self, layer):\n","        # Borrowed from official paper impl:\n","        # https://github.com/idiap/fullgrad-saliency/blob/master/saliency/tensor_extractor.py#L47\n","        if isinstance(layer, torch.nn.BatchNorm2d):\n","            bias = - (layer.running_mean * layer.weight\n","                      / torch.sqrt(layer.running_var + layer.eps)) + layer.bias\n","            return bias.data\n","        else:\n","            return layer.bias.data\n","\n","    def forward(self, input_tensor: torch.Tensor, targets: List[torch.nn.Module],\n","                eigen_smooth: bool = False) -> np.ndarray:\n","\n","        # print('\\n----------------------------------------Seg-Grad-Cam----------------------------------------------------\\n')\n","\n","        if self.cuda:\n","            input_tensor = input_tensor.cuda()\n","        if self.compute_input_gradient:\n","            input_tensor = torch.autograd.Variable(input_tensor, requires_grad=True)\n","\n","        outputs = self.activations_and_grads(input_tensor)\n","        if targets is None:\n","            target_categories = np.argmax(outputs.cpu().data.numpy(), axis=-1)\n","            targets = [ClassifierOutputTarget(category) for category in target_categories]\n","\n","        if self.uses_gradients:\n","            self.model.zero_grad()\n","            loss = sum([target(output) for target, output in zip(targets, outputs)])\n","            loss.backward(retain_graph=True)\n","\n","        #----remove------\n","\n","        input_grad = input_tensor.grad.data.cpu().numpy()\n","        grads_list = [g.cpu().data.numpy() for g in self.activations_and_grads.gradients]\n","        cam_per_target_layer = []\n","        target_size = input_tensor.size(-1), input_tensor.size(-2)\n","\n","        gradient_multiplied_input = input_grad * input_tensor.data.cpu().numpy()\n","        gradient_multiplied_input = np.abs(gradient_multiplied_input)\n","        gradient_multiplied_input = scale_accross_batch_and_channels(\n","            gradient_multiplied_input,\n","            target_size)\n","        cam_per_target_layer.append(gradient_multiplied_input)\n","\n","        # Loop over the saliency image from every layer\n","        assert(len(self.bias_data) == len(grads_list))\n","        for bias, grads in zip(self.bias_data, grads_list):\n","            bias = bias[None, :, None, None]\n","            # In the paper they take the absolute value,\n","            # but possibily taking only the positive gradients will work\n","            # better.\n","            bias_grad = np.abs(bias * grads)\n","            result = scale_accross_batch_and_channels(\n","                bias_grad, target_size)\n","            result = np.sum(result, axis=1)\n","            cam_per_target_layer.append(result[:, None, :])\n","        cam_per_target_layer = np.concatenate(cam_per_target_layer, axis=1)\n","        if eigen_smooth:\n","            # Resize to a smaller image, since this method typically has a very large number of channels,\n","            # and then consumes a lot of memory\n","            cam_per_target_layer = scale_accross_batch_and_channels(\n","                cam_per_target_layer, (target_size[0] // 8, target_size[1] // 8))\n","            cam_per_target_layer = get_2d_projection(cam_per_target_layer)\n","            cam_per_target_layer = cam_per_target_layer[:, None, :, :]\n","            cam_per_target_layer = scale_accross_batch_and_channels(\n","                cam_per_target_layer,\n","                target_size)\n","        else:\n","            cam_per_target_layer = np.sum(\n","                cam_per_target_layer, axis=1)[:, None, :]\n","\n","\n","        result = np.sum(cam_per_target_layer, axis=1)\n","        return scale_cam_image(result) # result\n","\n","    def __call__(self, input_tensor: torch.Tensor, targets: List[torch.nn.Module] = None, aug_smooth: bool = False,\n","                 eigen_smooth: bool = False) -> np.ndarray:\n","        # Smooth the CAM result with test time augmentation\n","        if aug_smooth is True:\n","            transforms = tta.Compose(\n","                [\n","                    tta.HorizontalFlip(),\n","                    tta.Multiply(factors=[0.9, 1, 1.1]),\n","                ]\n","            )\n","            cams = []\n","            for transform in transforms:\n","                augmented_tensor = transform.augment_image(input_tensor)\n","                cam = self.forward(augmented_tensor, targets, eigen_smooth)\n","                # The ttach library expects a tensor of size BxCxHxW\n","                cam = cam[:, None, :, :]\n","                cam = torch.from_numpy(cam)\n","                cam = transform.deaugment_mask(cam)\n","                # Back to numpy float32, HxW\n","                cam = cam.numpy()\n","                cam = cam[:, 0, :, :]\n","                cams.append(cam)\n","            cam = np.mean(np.float32(cams), axis=0)\n","            return cam\n","        else:\n","            return self.forward(input_tensor, targets, eigen_smooth)\n","\n","    def __del__(self):\n","        self.activations_and_grads.release()\n","\n","    def __enter__(self):\n","        return self\n","\n","    def __exit__(self, exc_type, exc_value, exc_tb):\n","        self.activations_and_grads.release()\n","        if isinstance(exc_value, IndexError):\n","            # Handle IndexError here...\n","            print(\n","                f\"An exception occurred in CAM with block: {exc_type}. Message: {exc_value}\")\n","            return True"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/","output_embedded_package_id":"1ayPCcBllQ89OB89T9eG_ia1lafPPyYhg","height":1000},"id":"8qvw_iUTsqWh","outputId":"47ec12d5-de36-45b0-d610-73fb5b27cd65","executionInfo":{"status":"ok","timestamp":1747931408020,"user_tz":-180,"elapsed":676604,"user":{"displayName":"Mohamad Sandakli","userId":"16501406067787434423"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["# Apply the adapted CAM-based Extensions on the considered dataset\n","\n","class SemanticSegmentationTarget:\n","    def __init__(self, category, mask):\n","        self.category = category\n","        self.mask = torch.from_numpy(mask)\n","        if torch.cuda.is_available():\n","            self.mask = self.mask.cuda()\n","\n","    def __call__(self, model_output):\n","        return (model_output[self.category, :, : ] * self.mask).sum()\n","\n","\n","def defaultScales():\n","    classes_cmap = plt.get_cmap('Spectral', 20)\n","    scale_fig = 2\n","    fonts = 15\n","    scatter_size = 330 * scale_fig\n","    return classes_cmap, scale_fig, fonts, scatter_size\n","\n","\n","def show_cam_image_whu(full_img_rgba, grayscale_cam_EX, image_weight):\n","    heatmap_sgc = cv2.applyColorMap(np.uint8(255 * grayscale_cam_EX), cv2.COLORMAP_JET)\n","    heatmap_sgc = cv2.cvtColor(heatmap_sgc, cv2.COLOR_BGR2RGB)\n","    heatmap_sgc = np.float32(heatmap_sgc) / 255\n","    if full_img.shape[-1]==4:\n","        heatmap_sgc = cv2.cvtColor(heatmap_sgc,cv2.COLOR_RGB2RGBA)\n","    Exmap_sgc = (1 - image_weight) * heatmap_sgc + image_weight * full_img_rgba\n","    Exmap_sgc = Exmap_sgc / np.max(Exmap_sgc)\n","    Exmap_sgc = np.uint8(255 * Exmap_sgc)\n","\n","    return Exmap_sgc, heatmap_sgc\n","\n","\n","def prob_2_entropy(prob):\n","    \"\"\" convert probabilistic prediction maps to weighted self-information maps\n","    \"\"\"\n","    n, c, h, w = prob.size()\n","    return -torch.mul(prob, torch.log2(prob + 1e-30)) / np.log2(c)\n","\n","\n","def XAI_EVAL(E_sgc, full_img_gt, model, rrp_info,target_category):\n","\n","    x_sgc = totensor(E_sgc)\n","    # x_sgc = x_sgc.cuda()\n","    with torch.no_grad():\n","        y_pred_sgc = model(x_sgc)\n","        y_pred_sgc = unpad_resize(y_pred_sgc,rrp_info)\n","\n","    mask_tensor_sgc = y_pred_sgc[0,...]\n","    mask_sgc = y_pred_sgc[0,...].cpu().numpy().transpose(1,2,0)\n","    target_mask_f = np.float32(mask_sgc[:,:,target_category]) * full_img_gt\n","    target_Confidence_score = target_mask_f[np.nonzero(target_mask_f)]\n","    target_Confidence_score_sgc = np.mean(target_Confidence_score)\n","\n","    logist_softmax_entropy_sgc = prob_2_entropy(y_pred_sgc)\n","    target_entropy_mask_sgc = logist_softmax_entropy_sgc[0, :, :, :].detach().cpu().numpy().transpose(1,2,0)\n","    target_entropy_mask_class_sgc = target_entropy_mask_sgc[:,:,target_category] * full_img_gt\n","    target_entropy_mask_class_sgc_sc = (np.mean(target_entropy_mask_class_sgc))\n","    return target_Confidence_score_sgc, target_entropy_mask_class_sgc_sc\n","\n","\n","images_dir = r'./dataset/images'\n","gt_dir = r'./dataset/gt'\n","\n","\n","\n","\n","\n","ticks = np.linspace(0, 1, 6, endpoint=True)\n","classes_cmap, scale_fig, fonts, scatter_size = defaultScales()\n","\n","\n","\n","\n","decoder_idx = 1\n","thres = 0.4\n","n_xai = 6\n","n_imgs = 1\n","target_layers =  [model.decoder.blocks[decoder_idx - 1]]\n","target_category = 0\n","XAI_method = [\"grad_cam\", \"hires_cam\", \"ew_cam\", \"grad_cam_pp\", \"x_grad_cam\",\"score_cam\", \"layer_cam\", \"eigen_cam\", \"eigen_grad_cam\"]\n","\n","image_weight = 0.006\n","\n","directory_images = os.fsencode(images_dir)\n","directory_gt = os.fsencode(gt_dir)\n","\n","\n","number_testing_images = 0\n","\n","Model_Seg_Score = np.zeros((n_imgs))\n","Model_Seg_Entropy = np.zeros((n_imgs))\n","\n","Seg_Score_M1 = np.zeros((n_xai, n_imgs))\n","Seg_Entropy_M1 = np.zeros((n_xai, n_imgs))\n","\n","Seg_Score_M2 = np.zeros((n_xai, n_imgs))\n","Seg_Entropy_M2 = np.zeros((n_xai, n_imgs))\n","\n","Seg_Score_M3 = np.zeros((n_xai, n_imgs))\n","Seg_Entropy_M3 = np.zeros((n_xai, n_imgs))\n","\n","counter = 0\n","\n","\n","for (file_img, file_gt) in zip(os.listdir(directory_images), os.listdir(directory_gt) ):\n","\n","    print(file_img, file_gt)\n","    if file_img == file_gt:\n","        number_testing_images = number_testing_images + 1\n","        print(\"Testing Image: \", number_testing_images)\n","        filename = os.fsdecode(file_img)\n","        filename_gt = os.fsdecode(file_gt)\n","        if filename.endswith(\".png\") and filename_gt.endswith(\".png\"):\n","            raster_file = rio.open(f'{images_dir}/{filename}')\n","            full_img = raster_file.read().transpose(1,2,0)\n","            full_img,rrp_info = ratio_resize_pad(full_img, ratio = None)\n","            full_img_rgba = full_img\n","            print(\"Input Image Shape: \", full_img.shape)\n","            if full_img.shape[-1]==4:full_img = cv2.cvtColor(full_img,cv2.COLOR_RGBA2RGB) #WHU images are RGBA\n","            # read gt mask\n","            raster_file_gt = rio.open(f'{gt_dir}/{filename_gt}')\n","            full_img_gt = raster_file_gt.read().transpose(1,2,0)\n","            full_img_gt,rrp_info_gt = ratio_resize_pad(full_img_gt, ratio = None)\n","            full_img_gt = np.float32(full_img_gt) / np.max(full_img_gt)\n","            print(\"GT mask shape: \", full_img_gt.shape)\n","\n","\n","            full_img = normalize(full_img)\n","            x = totensor(full_img)\n","            # x = x.cuda()\n","\n","            with torch.no_grad():\n","                y_pred = model(x)\n","                y_pred = unpad_resize(y_pred,rrp_info)\n","\n","\n","\n","            mask_tensor = y_pred[0,...]\n","            print('predicated tensor shape: ', mask_tensor.shape)\n","            mask = y_pred[0,...].cpu().numpy().transpose(1,2,0)\n","\n","            target_mask_float = np.float32(mask[:,:,target_category]) * full_img_gt\n","            target_Confidence_score = target_mask_float[np.nonzero(target_mask_float)]\n","            target_Confidence_score = np.mean(target_Confidence_score)\n","            print(\"Model Target Confidence Score: \", target_Confidence_score)\n","\n","            logist_softmax_entropy = prob_2_entropy(y_pred)\n","            target_entropy_mask = logist_softmax_entropy[0, :, :, :].detach().cpu().numpy().transpose(1,2,0)\n","            target_entropy_mask_class = target_entropy_mask[:,:,target_category] * full_img_gt\n","            model_entropy = np.mean(target_entropy_mask_class)\n","            print(\"Model Target Entropy Score: \", model_entropy )\n","\n","\n","            targets = [SemanticSegmentationTarget(target_category, target_mask_float)]\n","\n","            nan_condition = np.count_nonzero(target_mask_float)\n","            if nan_condition != 0:\n","\n","                print('running seg-grad-cam...')\n","                with GRADCAM_Extensions(extension = XAI_method[0], model=model, target_layers=target_layers, use_cuda=torch.cuda.is_available()) as cam:\n","                    grayscale_cam_EX = cam(input_tensor=x, targets=targets)[0, :]\n","                    [Exmap_sgc, heatmap_sgc_rgba] = show_cam_image_whu(full_img_rgba, grayscale_cam_EX, image_weight)\n","\n","                    # Evaluation\n","                    im_bw_sgc = cv2.threshold(grayscale_cam_EX, thres, 1, cv2.THRESH_BINARY)[1]\n","                    # M1: Background Only\n","                    E_sgc_M1 = full_img * np.logical_not(im_bw_sgc)[..., None]\n","                    [confidence_sgc_M1, entropy_sgc_M1] = XAI_EVAL(E_sgc_M1, full_img_gt, model, rrp_info,target_category)\n","                    print(\"M1 -- SGC Target Confidence Score: \", confidence_sgc_M1,\" Entropy Score: \", entropy_sgc_M1)\n","\n","                    # M2: Highlighted Only\n","                    E_sgc_M2 = full_img * im_bw_sgc[..., None]\n","                    [confidence_sgc_M2, entropy_sgc_M2] = XAI_EVAL(E_sgc_M2, full_img_gt, model, rrp_info,target_category)\n","                    print(\"M2 -- SGC Target Confidence Score: \", confidence_sgc_M2,\" Entropy Score: \", entropy_sgc_M2)\n","\n","                    # M3 : Highlighted + GT\n","                    union_gt_sgc = np.ma.mask_or(full_img_gt,im_bw_sgc)\n","                    E_sgc_M3 = full_img * union_gt_sgc[..., None]\n","\n","                    [confidence_sgc_M3, entropy_sgc_M3] = XAI_EVAL(E_sgc_M3, full_img_gt, model, rrp_info,target_category)\n","                    print(\"M3 -- SGC Target Confidence Score: \", confidence_sgc_M3,\" Entropy Score: \", entropy_sgc_M3)\n","                #______________________________\n","                print('running GradCam++ ...')\n","                with GRADCAM_Extensions(extension = XAI_method[3], model=model, target_layers=target_layers, use_cuda=torch.cuda.is_available()) as campp:\n","                    grayscale_cam_EX_Plusplus = campp(input_tensor=x, targets=targets)[0, :]\n","                    [Exmap_sgcpp, heatmap_sgcpp_rgba] = show_cam_image_whu(full_img_rgba, grayscale_cam_EX_Plusplus, image_weight)\n","\n","                \t\t# Evaluation\n","                    im_bw_sgcpp = cv2.threshold(grayscale_cam_EX_Plusplus, thres, 1, cv2.THRESH_BINARY)[1]\n","\n","                    # M1: Background Only\n","                    E_sgcpp_M1 = full_img * np.logical_not(im_bw_sgcpp)[..., None]\n","                    [confidence_sgcpp_M1, entropy_sgcpp_M1] = XAI_EVAL(E_sgcpp_M1, full_img_gt, model, rrp_info,target_category)\n","                    print(\"M1 -- SGC++ Target Confidence Score: \", confidence_sgcpp_M1,\" Entropy Score: \", entropy_sgcpp_M1)\n","\n","\t\t\t\t\t\t\t\t\t\t# M2: Highlighted Only\n","                    E_sgcpp_M2 = full_img * im_bw_sgcpp[..., None]\n","                    [confidence_sgcpp_M2, entropy_sgcpp_M2] = XAI_EVAL(E_sgcpp_M2, full_img_gt, model, rrp_info,target_category)\n","                    print(\"M2 -- SGC++ Target Confidence Score: \", confidence_sgcpp_M2,\" Entropy Score: \", entropy_sgcpp_M2)\n","\n","              \t\t\t# M3: Highlighted + GT\n","                    union_gt_sgcpp = np.ma.mask_or(full_img_gt,im_bw_sgcpp)\n","                    E_sgcpp_M3 = full_img * union_gt_sgcpp[..., None]\n","                    [confidence_sgcpp_M3, entropy_sgcpp_M3] = XAI_EVAL(E_sgcpp_M3, full_img_gt, model, rrp_info,target_category)\n","                    print(\"M3 -- SGC++ Target Confidence Score: \", confidence_sgcpp_M3,\" Entropy Score: \", entropy_sgcpp_M3)\n","\n","\t\t\t\t\t\t\t\t#___________________________\n","                print('running XGradCam...')\n","                with GRADCAM_Extensions(extension = XAI_method[4], model=model, target_layers=target_layers, use_cuda=torch.cuda.is_available()) as xcam:\n","                    grayscale_cam_EX_X = xcam(input_tensor=x, targets=targets)[0, :]\n","                    [Exmap_Xsgc, heatmap_Xsgc_rgba] = show_cam_image_whu(full_img_rgba, grayscale_cam_EX_X, image_weight)\n","\t\t\t\t\t\t\t\t\t\t# Evaluation\n","                    im_bw_Xsgc = cv2.threshold(grayscale_cam_EX_X, thres, 1, cv2.THRESH_BINARY)[1]\n","\n","                    # M1: Background Only\n","                    E_Xsgc_M1 = full_img * np.logical_not(im_bw_Xsgc)[..., None]\n","                    [confidence_Xsgc_M1, entropy_Xsgc_M1] = XAI_EVAL(E_Xsgc_M1, full_img_gt, model, rrp_info,target_category)\n","                    print(\"M1 -- XSGC Target Confidence Score: \", confidence_Xsgc_M1,\" Entropy Score: \", entropy_Xsgc_M1)\n","\n","\t\t\t\t\t\t\t\t\t\t# M2: Highlighted Only\n","                    E_Xsgc_M2 = full_img * im_bw_Xsgc[..., None]\n","                    [confidence_Xsgc_M2, entropy_Xsgc_M2] = XAI_EVAL(E_Xsgc_M2, full_img_gt, model, rrp_info,target_category)\n","                    print(\"M2 -- XSGC Target Confidence Score: \", confidence_Xsgc_M2,\" Entropy Score: \", entropy_Xsgc_M2)\n","\n","              \t\t\t# M3: Highlighted + GT\n","                    union_gt_Xsgc = np.ma.mask_or(full_img_gt,im_bw_Xsgc)\n","                    E_Xsgc_M3 = full_img * union_gt_Xsgc[..., None]\n","                    [confidence_Xsgc_M3, entropy_Xsgc_M3] = XAI_EVAL(E_Xsgc_M3, full_img_gt, model, rrp_info,target_category)\n","                    print(\"M3 -- XSGC Target Confidence Score: \", confidence_Xsgc_M3,\" Entropy Score: \", entropy_Xsgc_M3)\n","\n","\t\t\t\t\t\t\t\t#___________________________\n","                print('running ScoreCam...')\n","                with GRADCAM_Extensions(extension = XAI_method[5], model=model, target_layers=target_layers, use_cuda=torch.cuda.is_available()) as scorecam:\n","                    grayscale_cam_EX_Score = scorecam(input_tensor=x, targets=targets)[0, :]\n","                    [Exmap_scoresgc, heatmap_scoresgc_rgba] = show_cam_image_whu(full_img_rgba, grayscale_cam_EX_Score, image_weight)\n","\n","                    # Evaluation\n","                    im_bw_ssgc = cv2.threshold(grayscale_cam_EX_Score, thres, 1, cv2.THRESH_BINARY)[1]\n","\n","                    # M1: Background Only\n","                    E_ssgc_M1 = full_img * np.logical_not(im_bw_ssgc)[..., None]\n","                    [confidence_ssgc_M1, entropy_ssgc_M1] = XAI_EVAL(E_ssgc_M1, full_img_gt, model, rrp_info,target_category)\n","                    print(\"M1 -- Score SGC Target Confidence Score: \", confidence_ssgc_M1,\" Entropy Score: \", entropy_ssgc_M1)\n","\n","\t\t\t\t\t\t\t\t\t\t# M2: Highlighted Only\n","                    E_ssgc_M2 = full_img * im_bw_ssgc[..., None]\n","                    [confidence_ssgc_M2, entropy_ssgc_M2] = XAI_EVAL(E_ssgc_M2, full_img_gt, model, rrp_info,target_category)\n","                    print(\"M2 -- Score SGC Target Confidence Score: \", confidence_ssgc_M2,\" Entropy Score: \", entropy_ssgc_M2)\n","\n","              \t\t\t# M3: Highlighted + GT\n","                    union_gt_ssgc = np.ma.mask_or(full_img_gt,im_bw_ssgc)\n","                    E_ssgc_M3 = full_img * union_gt_ssgc[..., None]\n","                    [confidence_ssgc_M3, entropy_ssgc_M3] = XAI_EVAL(E_ssgc_M3, full_img_gt, model, rrp_info,target_category)\n","                    print(\"M3 -- Score SGC Target Confidence Score: \", confidence_ssgc_M3,\" Entropy Score: \", entropy_ssgc_M3)\n","\n","\t\t\t\t\t\t\t\t#__________________________\n","                print('running EigenCam...')\n","                with GRADCAM_Extensions(extension = XAI_method[7], model=model, target_layers=target_layers, use_cuda=torch.cuda.is_available()) as ecam:\n","                    grayscale_cam_EX_eigen = ecam(input_tensor=x, targets=targets)[0, :]\n","                    [Exmap_eigensgc, heatmap_eigensgc_rgba] = show_cam_image_whu(full_img_rgba, grayscale_cam_EX_eigen, image_weight)\n","\n","\t\t\t\t\t\t\t\t\t\t# Evaluation\n","                    im_bw_esgc = cv2.threshold(grayscale_cam_EX_eigen, thres, 1, cv2.THRESH_BINARY)[1]\n","\n","                    # M1: Background Only\n","                    E_esgc_M1 = full_img * np.logical_not(im_bw_esgc)[..., None]\n","                    [confidence_esgc_M1, entropy_esgc_M1] = XAI_EVAL(E_esgc_M1, full_img_gt, model, rrp_info,target_category)\n","                    print(\"M1 -- Eigen SGC Target Confidence Score: \", confidence_esgc_M1,\" Entropy Score: \", entropy_esgc_M1)\n","\n","\t\t\t\t\t\t\t\t\t\t# M2: Highlighted Only\n","                    E_esgc_M2 = full_img * im_bw_esgc[..., None]\n","                    [confidence_esgc_M2, entropy_esgc_M2] = XAI_EVAL(E_esgc_M2, full_img_gt, model, rrp_info,target_category)\n","                    print(\"M2 -- Eigen SGC Target Confidence Score: \", confidence_esgc_M2,\" Entropy Score: \", entropy_esgc_M2)\n","\n","              \t\t\t# M3: Highlighted + GT\n","                    union_gt_esgc = np.ma.mask_or(full_img_gt,im_bw_esgc)\n","                    E_esgc_M3 = full_img * union_gt_esgc[..., None]\n","                    [confidence_esgc_M3, entropy_esgc_M3] = XAI_EVAL(E_esgc_M3, full_img_gt, model, rrp_info,target_category)\n","                    print(\"M3 -- Eigen SGC Target Confidence Score: \", confidence_esgc_M3,\" Entropy Score: \", entropy_esgc_M3)\n","\n","                Model_Seg_Score[counter] = target_Confidence_score\n","                Model_Seg_Entropy[counter] = model_entropy\n","\n","                Seg_Score_M1[0,counter] = confidence_sgc_M1\n","                Seg_Score_M1[1,counter] = confidence_sgcpp_M1\n","                Seg_Score_M1[2,counter] = confidence_Xsgc_M1\n","                Seg_Score_M1[3,counter] = confidence_ssgc_M1\n","                Seg_Score_M1[4,counter] = confidence_esgc_M1\n","\n","                Seg_Entropy_M1[0,counter] = entropy_sgc_M1\n","                Seg_Entropy_M1[1,counter] = entropy_sgcpp_M1\n","                Seg_Entropy_M1[2,counter] = entropy_Xsgc_M1\n","                Seg_Entropy_M1[3,counter] = entropy_ssgc_M1\n","                Seg_Entropy_M1[4,counter] = entropy_esgc_M1\n","\n","\n","                Seg_Score_M2[0,counter] = confidence_sgc_M2\n","                Seg_Score_M2[1,counter] = confidence_sgcpp_M2\n","                Seg_Score_M2[2,counter] = confidence_Xsgc_M2\n","                Seg_Score_M2[3,counter] = confidence_ssgc_M2\n","                Seg_Score_M2[4,counter] = confidence_esgc_M2\n","\n","                Seg_Entropy_M2[0,counter] = entropy_sgc_M2\n","                Seg_Entropy_M2[1,counter] = entropy_sgcpp_M2\n","                Seg_Entropy_M2[2,counter] = entropy_Xsgc_M2\n","                Seg_Entropy_M2[3,counter] = entropy_ssgc_M2\n","                Seg_Entropy_M2[4,counter] = entropy_esgc_M2\n","\n","                Seg_Score_M3[0,counter] = confidence_sgc_M3\n","                Seg_Score_M3[1,counter] = confidence_sgcpp_M3\n","                Seg_Score_M3[2,counter] = confidence_Xsgc_M3\n","                Seg_Score_M3[3,counter] = confidence_ssgc_M3\n","                Seg_Score_M3[4,counter] = confidence_esgc_M3\n","\n","                Seg_Entropy_M3[0,counter] = entropy_sgc_M3\n","                Seg_Entropy_M3[1,counter] = entropy_sgcpp_M3\n","                Seg_Entropy_M3[2,counter] = entropy_Xsgc_M3\n","                Seg_Entropy_M3[3,counter] = entropy_ssgc_M3\n","                Seg_Entropy_M3[4,counter] = entropy_esgc_M3\n","\n","\n","                counter = counter + 1\n","\n","                if(number_testing_images == 2):\n","                    break\n","\n","\n","\n","                fig = plt.figure(figsize=(8 * scale_fig, 8 * scale_fig))\n","\n","                plt.subplot(3,3,1)\n","                plt.axis('off')\n","                plt.imshow(full_img_rgba)\n","                plt.title('Input image', fontsize=fonts)\n","\n","                plt.subplot(3,3,2)\n","                plt.axis('off')\n","                plt.imshow(full_img_gt, cmap='gray', vmin=0, vmax=1)\n","                plt.title('Ground Truth (GT) Mask', fontsize=fonts)\n","\n","                plt.subplot(3,3,3)\n","                plt.axis('off')\n","                plt.imshow(target_mask_float,  cmap='gray', vmin=0, vmax=1)\n","                plt.title('Predicted Mask', fontsize=fonts)\n","\n","                plt.subplot(3,3,4)\n","                plt.axis('off')\n","                plt.imshow(Exmap_sgc)\n","                plt.title('Seg-Grad-CAM', fontsize=fonts)\n","\n","                plt.subplot(3,3,5)\n","                plt.axis('off')\n","                plt.imshow(Exmap_sgcpp)\n","                plt.title('Seg-Grad-CAM ++', fontsize=fonts)\n","\n","                plt.subplot(3,3,6)\n","                plt.axis('off')\n","                plt.imshow(Exmap_Xsgc)\n","                plt.title('Seg-XGrad-CAM', fontsize=fonts)\n","\n","                plt.subplot(3,3,7)\n","                plt.axis('off')\n","                plt.imshow(Exmap_scoresgc)\n","                plt.title('Seg-Score-CAM', fontsize=fonts)\n","\n","                plt.subplot(3,3,8)\n","                plt.axis('off')\n","                plt.imshow(Exmap_eigensgc)\n","                plt.title('Seg-Eigen-CAM', fontsize=fonts)\n","\n","\n","                # plt.subplot(3,3,9)\n","                # plt.axis('off')\n","                # plt.imshow(Exmap_ablsgc)\n","                # plt.title('Seg-Ablation-CAM', fontsize=fonts)\n","\n","                # Save the full figure...\n","                fig.savefig('./results/SegGradCam_Extensions_{}_{}.png'.format(filename,decoder_idx), bbox_inches='tight', pad_inches=0)\n","\n","\n","\n","                fig = plt.figure(figsize=(8 * scale_fig, 8 * scale_fig))\n","\n","                plt.subplot(3,3,1)\n","                plt.axis('off')\n","                plt.imshow(full_img_rgba)\n","                plt.title('Input image', fontsize=fonts)\n","\n","                plt.subplot(3,3,2)\n","                plt.axis('off')\n","                plt.imshow(full_img_gt, cmap='gray', vmin=0, vmax=1)\n","                plt.title('Ground Truth (GT) Mask', fontsize=fonts)\n","\n","                plt.subplot(3,3,3)\n","                plt.axis('off')\n","                plt.imshow(target_mask_float,  cmap='gray', vmin=0, vmax=1)\n","                plt.title('Predicted Mask', fontsize=fonts)\n","\n","                plt.subplot(3,3,4)\n","                plt.axis('off')\n","                plt.imshow(E_sgc_M3)\n","                plt.title('Seg-Grad-CAM', fontsize=fonts)\n","\n","                plt.subplot(3,3,5)\n","                plt.axis('off')\n","                plt.imshow(E_sgcpp_M3)\n","                plt.title('Seg-Grad-CAM ++', fontsize=fonts)\n","\n","                plt.subplot(3,3,6)\n","                plt.axis('off')\n","                plt.imshow(E_Xsgc_M3)\n","                plt.title('Seg-XGrad-CAM', fontsize=fonts)\n","\n","                plt.subplot(3,3,7)\n","                plt.axis('off')\n","                plt.imshow(E_ssgc_M3)\n","                plt.title('Seg-Score-CAM', fontsize=fonts)\n","\n","                plt.subplot(3,3,8)\n","                plt.axis('off')\n","                plt.imshow(E_esgc_M3)\n","                plt.title('Seg-Eigen-CAM', fontsize=fonts)\n","\n","\n","                # plt.subplot(3,3,9)\n","                # plt.axis('off')\n","                # plt.imshow(Exmap_ablsgc)\n","                # plt.title('Seg-Ablation-CAM', fontsize=fonts)\n","\n","                # Save the full figure...\n","                fig.savefig('./results/SegGradCam_Modified_M3_{}_{}.png'.format(filename,decoder_idx), bbox_inches='tight', pad_inches=0)\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"hmjKb8CST2zb"},"source":["# Task 1:Evaluating XAI Methods based on M1, M2, M3 across different thresholds"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"XY4YcAaxr-VC","executionInfo":{"status":"ok","timestamp":1747931408444,"user_tz":-180,"elapsed":17,"user":{"displayName":"Mohamad Sandakli","userId":"16501406067787434423"}}},"outputs":[],"source":["XAI_method_t1 = [\"grad_cam\", \"grad_cam_pp\", \"x_grad_cam\", \"score_cam\", \"eigen_cam\"]\n","thresholds = [0.1, 0.2, 0.3, 0.4, 0.5]"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uCGFn7Ryr4Fo","executionInfo":{"status":"ok","timestamp":1747932111624,"user_tz":-180,"elapsed":703155,"user":{"displayName":"Mohamad Sandakli","userId":"16501406067787434423"}},"outputId":"09ecf6a4-2dc9-4701-bcb1-9dac27001780"},"outputs":[{"output_type":"stream","name":"stdout","text":["Running grad_cam ...\n","Running grad_cam_pp ...\n","Running x_grad_cam ...\n","Running score_cam ...\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 32/32 [10:18<00:00, 19.32s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Running eigen_cam ...\n"]}],"source":["results_t1 = {\n","    method: {\n","        th: {'M1': {}, 'M2': {}, 'M3': {}} for th in thresholds\n","    }\n","    for method in XAI_method_t1\n","}\n","\n","for method_name in XAI_method_t1:\n","    print(f'Running {method_name} ...')\n","\n","    with GRADCAM_Extensions(extension=method_name, model=model, target_layers=target_layers, use_cuda=torch.cuda.is_available()) as cam:\n","        grayscale_cam = cam(input_tensor=x, targets=targets)[0, :]\n","\n","        for thres in thresholds:\n","\n","            im_bw = cv2.threshold(grayscale_cam, thres, 1, cv2.THRESH_BINARY)[1]\n","\n","            # M1: Background Only\n","            E_M1 = full_img * np.logical_not(im_bw)[..., None]\n","            confidence_M1, entropy_M1 = XAI_EVAL(E_M1, full_img_gt, model, rrp_info, target_category)\n","\n","            # M2: Highlighted Only\n","            E_M2 = full_img * im_bw[..., None]\n","            confidence_M2, entropy_M2 = XAI_EVAL(E_M2, full_img_gt, model, rrp_info, target_category)\n","\n","            # M3: Highlighted + GT\n","            union_mask = np.ma.mask_or(full_img_gt, im_bw)\n","            E_M3 = full_img * union_mask[..., None]\n","            confidence_M3, entropy_M3 = XAI_EVAL(E_M3, full_img_gt, model, rrp_info, target_category)\n","\n","            results_t1[method_name][thres]['M1'] = {'confidence': confidence_M1, 'entropy': entropy_M1}\n","            results_t1[method_name][thres]['M2'] = {'confidence': confidence_M2, 'entropy': entropy_M2}\n","            results_t1[method_name][thres]['M3'] = {'confidence': confidence_M3, 'entropy': entropy_M3}\n"]},{"cell_type":"markdown","metadata":{"id":"_pWKOUZA2BrW"},"source":["**Convert results into dataframe to print**"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9towB2y0Xjy8","executionInfo":{"status":"ok","timestamp":1747932111661,"user_tz":-180,"elapsed":60,"user":{"displayName":"Mohamad Sandakli","userId":"16501406067787434423"}},"outputId":"19780fe0-f48d-4dbf-92bf-62ca638620b3"},"outputs":[{"output_type":"stream","name":"stdout","text":["     Method  Threshold Metric  Confidence  Entropy\n","  eigen_cam        0.1     M1    0.911135 0.001452\n","  eigen_cam        0.1     M2    0.207058 0.003010\n","  eigen_cam        0.1     M3    0.147083 0.002163\n","  eigen_cam        0.2     M1    0.911754 0.001386\n","  eigen_cam        0.2     M2    0.575863 0.005440\n","  eigen_cam        0.2     M3    0.059528 0.001578\n","  eigen_cam        0.3     M1    0.910741 0.001438\n","  eigen_cam        0.3     M2    0.596144 0.005520\n","  eigen_cam        0.3     M3    0.054794 0.001619\n","  eigen_cam        0.4     M1    0.911137 0.001412\n","  eigen_cam        0.4     M2    0.656418 0.005555\n","  eigen_cam        0.4     M3    0.042779 0.001307\n","  eigen_cam        0.5     M1    0.911723 0.001389\n","  eigen_cam        0.5     M2    0.731848 0.005596\n","  eigen_cam        0.5     M3    0.035465 0.001225\n","   grad_cam        0.1     M1    0.878462 0.002150\n","   grad_cam        0.1     M2    0.161230 0.000952\n","   grad_cam        0.1     M3    0.357960 0.002641\n","   grad_cam        0.2     M1    0.894455 0.001875\n","   grad_cam        0.2     M2    0.181344 0.000795\n","   grad_cam        0.2     M3    0.068210 0.001522\n","   grad_cam        0.3     M1    0.884739 0.001860\n","   grad_cam        0.3     M2    0.591774 0.002913\n","   grad_cam        0.3     M3    0.035126 0.001178\n","   grad_cam        0.4     M1    0.894396 0.001875\n","   grad_cam        0.4     M2    0.725109 0.002610\n","   grad_cam        0.4     M3    0.033522 0.001117\n","   grad_cam        0.5     M1    0.907990 0.001611\n","   grad_cam        0.5     M2    0.768169 0.002974\n","   grad_cam        0.5     M3    0.033522 0.001117\n","grad_cam_pp        0.1     M1    0.090199 0.001079\n","grad_cam_pp        0.1     M2    0.895930 0.001420\n","grad_cam_pp        0.1     M3    0.901705 0.001388\n","grad_cam_pp        0.2     M1    0.677298 0.003101\n","grad_cam_pp        0.2     M2    0.397959 0.001734\n","grad_cam_pp        0.2     M3    0.644473 0.002886\n","grad_cam_pp        0.3     M1    0.892724 0.001755\n","grad_cam_pp        0.3     M2    0.184199 0.000578\n","grad_cam_pp        0.3     M3    0.163283 0.001737\n","grad_cam_pp        0.4     M1    0.899026 0.001797\n","grad_cam_pp        0.4     M2    0.260340 0.001966\n","grad_cam_pp        0.4     M3    0.037998 0.001131\n","grad_cam_pp        0.5     M1    0.892636 0.001885\n","grad_cam_pp        0.5     M2    0.621567 0.002452\n","grad_cam_pp        0.5     M3    0.033526 0.001117\n","  score_cam        0.1     M1    0.118837 0.002417\n","  score_cam        0.1     M2    0.904449 0.001422\n","  score_cam        0.1     M3    0.904116 0.001410\n","  score_cam        0.2     M1    0.032427 0.000289\n","  score_cam        0.2     M2    0.898507 0.001354\n","  score_cam        0.2     M3    0.905012 0.001380\n","  score_cam        0.3     M1    0.039928 0.000661\n","  score_cam        0.3     M2    0.869302 0.001446\n","  score_cam        0.3     M3    0.889870 0.001384\n","  score_cam        0.4     M1    0.197047 0.001647\n","  score_cam        0.4     M2    0.616674 0.001916\n","  score_cam        0.4     M3    0.757955 0.001983\n","  score_cam        0.5     M1    0.535484 0.002970\n","  score_cam        0.5     M2    0.234115 0.001144\n","  score_cam        0.5     M3    0.615401 0.002633\n"," x_grad_cam        0.1     M1    0.783097 0.002202\n"," x_grad_cam        0.1     M2    0.339765 0.001500\n"," x_grad_cam        0.1     M3    0.468596 0.002080\n"," x_grad_cam        0.2     M1    0.903228 0.001773\n"," x_grad_cam        0.2     M2    0.189531 0.000537\n"," x_grad_cam        0.2     M3    0.098279 0.002027\n"," x_grad_cam        0.3     M1    0.894193 0.001901\n"," x_grad_cam        0.3     M2    0.326896 0.003272\n"," x_grad_cam        0.3     M3    0.035874 0.001184\n"," x_grad_cam        0.4     M1    0.890874 0.001903\n"," x_grad_cam        0.4     M2    0.606105 0.003162\n"," x_grad_cam        0.4     M3    0.033650 0.001117\n"," x_grad_cam        0.5     M1    0.901858 0.001728\n"," x_grad_cam        0.5     M2    0.756821 0.001987\n"," x_grad_cam        0.5     M3    0.033522 0.001117\n"]}],"source":["rows = []\n","for method in results_t1:\n","    for th in results_t1[method]:\n","        for metric in results_t1[method][th]:\n","            row = {\n","                'Method': method,\n","                'Threshold': th,\n","                'Metric': metric,\n","                'Confidence': results_t1[method][th][metric]['confidence'],\n","                'Entropy': results_t1[method][th][metric]['entropy']\n","            }\n","            rows.append(row)\n","\n","df_results_t1 = pd.DataFrame(rows)\n","\n","df_results_t1 = df_results_t1.sort_values(by=['Method', 'Threshold', 'Metric'])\n","\n","print(df_results_t1.to_string(index=False))"]},{"cell_type":"markdown","metadata":{"id":"7ZK1IKrizI5L"},"source":["**Best XAI Method for each Threshold for each Evaluation Metric**"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ctmgu6BfkU45","executionInfo":{"status":"ok","timestamp":1747932111813,"user_tz":-180,"elapsed":143,"user":{"displayName":"Mohamad Sandakli","userId":"16501406067787434423"}},"outputId":"795ae4df-cd27-41b8-b36a-1f19da027cca"},"outputs":[{"output_type":"stream","name":"stdout","text":["    Threshold Metric       Method  Confidence   Entropy\n","0         0.1     M1  grad_cam_pp    0.090199  0.001079\n","1         0.2     M1    score_cam    0.032427  0.000289\n","2         0.3     M1    score_cam    0.039928  0.000661\n","3         0.4     M1    score_cam    0.197047  0.001647\n","4         0.5     M1    score_cam    0.535484  0.002970\n","5         0.1     M2    score_cam    0.904449  0.001422\n","6         0.2     M2    score_cam    0.898507  0.001354\n","7         0.3     M2    score_cam    0.869302  0.001446\n","8         0.4     M2     grad_cam    0.725109  0.002610\n","9         0.5     M2     grad_cam    0.768169  0.002974\n","10        0.1     M3  grad_cam_pp    0.901705  0.001388\n","11        0.2     M3    score_cam    0.905012  0.001380\n","12        0.3     M3     grad_cam    0.035126  0.001178\n","13        0.4     M3   x_grad_cam    0.033650  0.001117\n","14        0.5     M3  grad_cam_pp    0.033526  0.001117\n"]}],"source":["best_methods = []\n","\n","for (thresh, metric), group in df_results_t1.groupby(['Threshold', 'Metric']):\n","    if metric == 'M1':\n","        # Lowest confidence\n","        best_row = group.loc[group['Confidence'].idxmin()]\n","    elif metric == 'M2':\n","        # Highest confidence\n","        best_row = group.loc[group['Confidence'].idxmax()]\n","    elif metric == 'M3':\n","        # Lowest entropy\n","        best_row = group.loc[group['Entropy'].idxmin()]\n","    else:\n","        continue\n","\n","    best_methods.append(best_row)\n","\n","best_methods_df = pd.DataFrame(best_methods)\n","\n","best_methods_df = best_methods_df.sort_values(['Metric', 'Threshold']).reset_index(drop=True)\n","print(best_methods_df[['Threshold', 'Metric', 'Method', 'Confidence', 'Entropy']])\n"]},{"cell_type":"markdown","metadata":{"id":"TrMzJGm5CMfP"},"source":["* **Lowest M1 Confidence:** Background alone should give low confidence (excluded irrelevant info)\n","* **Highest M2 Confidence:** Highlighted region alone should give high confidence (important info captured)\n","* **Lowest M3 Entropy:** Explanation aligns well with GT and model is confident"]},{"cell_type":"markdown","metadata":{"id":"zcU_xdEGy_Ni"},"source":["**Best Overall XAI Method is Most Frequent Best**"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ma-rbmBgk-aZ","executionInfo":{"status":"ok","timestamp":1747932111897,"user_tz":-180,"elapsed":89,"user":{"displayName":"Mohamad Sandakli","userId":"16501406067787434423"}},"outputId":"742f1b95-7c09-4475-f52f-d9577d24c5e2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Method Selection Frequency:\n","Method\n","score_cam      8\n","grad_cam_pp    3\n","grad_cam       3\n","x_grad_cam     1\n","Name: count, dtype: int64\n","\n","Best overall XAI method: score_cam\n"]}],"source":["method_score = best_methods_df['Method'].value_counts()\n","\n","print(\"Method Selection Frequency:\")\n","print(method_score)\n","\n","best_overall_method = method_score.idxmax()\n","print(f\"\\nBest overall XAI method: {best_overall_method}\")\n"]},{"cell_type":"markdown","metadata":{"id":"1L-5ML07ErMZ"},"source":["**Best Threshold Value(s)**"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MDEP_60xEnnF","executionInfo":{"status":"ok","timestamp":1747932209270,"user_tz":-180,"elapsed":280,"user":{"displayName":"Mohamad Sandakli","userId":"16501406067787434423"}},"outputId":"b5c250f1-1a0a-434b-ffdc-de71a8440b8a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Method: eigen_cam\n"," Best threshold: 0.4\n","\n","Method: grad_cam\n"," Best threshold: 0.3\n","\n","Method: grad_cam_pp\n"," Best threshold: 0.1\n","\n","Method: score_cam\n"," Best threshold: 0.2\n","\n","Method: x_grad_cam\n"," Best threshold: 0.5\n","\n"]}],"source":["def normalize(arr):\n","    return (arr - np.min(arr)) / (np.max(arr) - np.min(arr))\n","\n","best_thresholds = {}\n","\n","for method in df_results_t1['Method'].unique():\n","    df_m = df_results_t1[df_results_t1['Method'] == method]\n","\n","    conf_pivot = df_m.pivot(index='Threshold', columns='Metric', values='Confidence')\n","    entropy_pivot = df_m.pivot(index='Threshold', columns='Metric', values='Entropy')\n","\n","    m1_conf_norm = normalize(conf_pivot['M1'].values)\n","    m2_conf_norm = normalize(conf_pivot['M2'].values)\n","    m3_entropy_norm = normalize(entropy_pivot['M3'].values)\n","\n","    # Combined score: low M1 conf, high M2 conf, low M3 entropy\n","    combined_score = m1_conf_norm + (1 - m2_conf_norm) + m3_entropy_norm\n","\n","    best_idx = np.argmin(combined_score)\n","    best_thresh = conf_pivot.index[best_idx]\n","\n","    best_thresholds[method] = {\n","        'best_threshold': best_thresh,\n","        'combined_scores': dict(zip(conf_pivot.index, combined_score))\n","    }\n","\n","for method, info in best_thresholds.items():\n","    print(f\"Method: {method}\")\n","    print(f\" Best threshold: {info['best_threshold']}\")\n","    print()\n"]},{"cell_type":"markdown","metadata":{"id":"mk3Fk7YNF91C"},"source":["---\n","The heatmap threshold strongly influences the faithfulness and clarity of XAI explanations by controlling which pixels are considered important."]},{"cell_type":"markdown","metadata":{"id":"JNVU2D9-EiP0"},"source":["# Task 2: Drop in Segmentation Score Metric After Masking Important Regions"]},{"cell_type":"markdown","source":["IoU measures intersection over union of model's prediction on image and ground-truth mask\n","\n","Drop in IoU measures how much the model performs when we mask the img using xai to show only the parts it thinks are imp."],"metadata":{"id":"QeSgd2mkTJmW"}},{"cell_type":"code","execution_count":18,"metadata":{"id":"ivvf5pd1OTVF","executionInfo":{"status":"ok","timestamp":1747932216892,"user_tz":-180,"elapsed":18,"user":{"displayName":"Mohamad Sandakli","userId":"16501406067787434423"}}},"outputs":[],"source":["thres = 0.4"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"PmVnNtEcM3jF","executionInfo":{"status":"ok","timestamp":1747932220955,"user_tz":-180,"elapsed":47,"user":{"displayName":"Mohamad Sandakli","userId":"16501406067787434423"}}},"outputs":[],"source":["def XAI_EVAL_Drop(E_sgc, full_img_gt, model, full_img, rrp_info, target_category):\n","    import torch.nn.functional as F\n","    import numpy as np\n","\n","    def compute_iou(pred_mask, gt_mask):\n","        intersection = np.logical_and(pred_mask, gt_mask).sum()\n","        union = np.logical_or(pred_mask, gt_mask).sum()\n","        if union == 0: return 0.0\n","        return intersection / union\n","\n","    x_sgc = totensor(E_sgc)\n","    with torch.no_grad():\n","        # Prediction from masked input (explanation-applied)\n","        y_pred_sgc = model(x_sgc)\n","        y_pred_sgc = unpad_resize(y_pred_sgc, rrp_info)\n","\n","        # Prediction of full img\n","        x_full = totensor(full_img)\n","        y_pred_full = model(x_full)\n","        y_pred_full = unpad_resize(y_pred_full, rrp_info)\n","\n","    pred_sgc = torch.argmax(y_pred_sgc[0], dim=0).cpu().numpy()\n","    pred_full = torch.argmax(y_pred_full[0], dim=0).cpu().numpy()\n","    gt_mask = full_img_gt.astype(np.uint8)\n","\n","    iou_full = compute_iou(pred_full == target_category, gt_mask == target_category)\n","    iou_sgc = compute_iou(pred_sgc == target_category, gt_mask == target_category)\n","\n","    drop_in_iou = iou_full - iou_sgc\n","\n","    return drop_in_iou\n"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cCz22mngKmVm","executionInfo":{"status":"ok","timestamp":1747932874584,"user_tz":-180,"elapsed":650446,"user":{"displayName":"Mohamad Sandakli","userId":"16501406067787434423"}},"outputId":"a959f0f5-fd3a-4395-cf08-024fddc2ae0c"},"outputs":[{"output_type":"stream","name":"stdout","text":["running grad_cam ...\n","grad_cam Drop in Segmentation:  -0.23857735226199223\n","running grad_cam_pp ...\n","grad_cam_pp Drop in Segmentation:  -0.24065775090664665\n","running x_grad_cam ...\n","x_grad_cam Drop in Segmentation:  -0.23857641220598186\n","running score_cam ...\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 32/32 [10:18<00:00, 19.34s/it]\n"]},{"output_type":"stream","name":"stdout","text":["score_cam Drop in Segmentation:  -0.30579284561942155\n","running eigen_cam ...\n","eigen_cam Drop in Segmentation:  0.027292047602201046\n"]}],"source":["drop_results = {method: {} for method in XAI_method_t1}\n","\n","for method_name in XAI_method_t1:\n","    print(f'running {method_name} ...')\n","\n","    with GRADCAM_Extensions(extension=method_name, model=model, target_layers=target_layers, use_cuda=torch.cuda.is_available()) as cam:\n","        grayscale_cam = cam(input_tensor=x, targets=targets)[0, :]\n","\n","        im_bw = cv2.threshold(grayscale_cam, thres, 1, cv2.THRESH_BINARY)[1]\n","\n","        # M3: Highlighted + GT\n","        union_mask = np.ma.mask_or(full_img_gt, im_bw)\n","        E_img = full_img * union_mask[..., None]\n","        drop_result = XAI_EVAL_Drop(E_img, full_img_gt, model, full_img, rrp_info, target_category)\n","\n","        print(f\"{method_name} Drop in Segmentation: \", drop_result)\n","        drop_results[method_name] = {\n","            'drop': drop_result\n","        }\n"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iZEblaXb9tw-","executionInfo":{"status":"ok","timestamp":1747932874745,"user_tz":-180,"elapsed":136,"user":{"displayName":"Mohamad Sandakli","userId":"16501406067787434423"}},"outputId":"612b1c67-ac00-4c84-f227-0b71923d699d"},"outputs":[{"output_type":"stream","name":"stdout","text":["XAI Methods Ranked by Faithfulness to the model’s true reasoning:\n","1. score_cam: Drop = -0.3058\n","2. grad_cam_pp: Drop = -0.2407\n","3. grad_cam: Drop = -0.2386\n","4. x_grad_cam: Drop = -0.2386\n","5. eigen_cam: Drop = 0.0273\n"]}],"source":["ranked_methods_t2 = sorted(drop_results.items(), key=lambda x: x[1]['drop'])\n","\n","print(\"XAI Methods Ranked by Faithfulness to the model’s true reasoning:\")\n","for i, (method, result) in enumerate(ranked_methods_t2, start=1):\n","    print(f\"{i}. {method}: Drop = {result['drop']:.4f}\")\n"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GsmV3kBG-vOD","executionInfo":{"status":"ok","timestamp":1747932888213,"user_tz":-180,"elapsed":75,"user":{"displayName":"Mohamad Sandakli","userId":"16501406067787434423"}},"outputId":"b16ca9a5-e724-4894-fa82-71e54c0f1e14"},"outputs":[{"output_type":"stream","name":"stdout","text":[" Best XAI method (lowest drop in IoU): score_cam with drop = -0.3058\n"]}],"source":["best_method_t2, best_result_t2 = ranked_methods_t2[0]\n","\n","print(f\" Best XAI method (lowest drop in IoU): {best_method_t2} with drop = {best_result_t2['drop']:.4f}\")\n"]},{"cell_type":"markdown","source":["---\n","When applying the M3 evaluation, a lower IoU drop indicates that the explanation method successfully highlighted the regions most critical to the model’s segmentation of the target class, even beyond the ground truth boundaries—demonstrating higher faithfulness to the model’s reasoning"],"metadata":{"id":"8dfZjtVM0gRL"}},{"cell_type":"markdown","metadata":{"id":"IYHTh9ADTfVD"},"source":["# Task 3: Using 2nd Decoder"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1uyRGMy36ZXpH9f4Vo_g2fb6i9uNGqW2l"},"id":"yNkkNJUDXSl7","executionInfo":{"status":"ok","timestamp":1747933261123,"user_tz":-180,"elapsed":362368,"user":{"displayName":"Mohamad Sandakli","userId":"16501406067787434423"}},"outputId":"829899a6-fd44-43ba-ff53-162f709e7a59"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["class SemanticSegmentationTarget:\n","    def __init__(self, category, mask):\n","        self.category = category\n","        self.mask = torch.from_numpy(mask)\n","        if torch.cuda.is_available():\n","            self.mask = self.mask.cuda()\n","\n","    def __call__(self, model_output):\n","        return (model_output[self.category, :, : ] * self.mask).sum()\n","\n","\n","def defaultScales():\n","    classes_cmap = plt.get_cmap('Spectral', 20)\n","    scale_fig = 2\n","    fonts = 15\n","    scatter_size = 330 * scale_fig\n","    return classes_cmap, scale_fig, fonts, scatter_size\n","\n","\n","def show_cam_image_whu(full_img_rgba, grayscale_cam_EX, image_weight):\n","    heatmap_sgc = cv2.applyColorMap(np.uint8(255 * grayscale_cam_EX), cv2.COLORMAP_JET)\n","    heatmap_sgc = cv2.cvtColor(heatmap_sgc, cv2.COLOR_BGR2RGB)\n","    heatmap_sgc = np.float32(heatmap_sgc) / 255\n","    if full_img.shape[-1]==4:\n","        heatmap_sgc = cv2.cvtColor(heatmap_sgc,cv2.COLOR_RGB2RGBA)\n","    Exmap_sgc = (1 - image_weight) * heatmap_sgc + image_weight * full_img_rgba\n","    Exmap_sgc = Exmap_sgc / np.max(Exmap_sgc)\n","    Exmap_sgc = np.uint8(255 * Exmap_sgc)\n","\n","    return Exmap_sgc, heatmap_sgc\n","\n","\n","def prob_2_entropy(prob):\n","    \"\"\" convert probabilistic prediction maps to weighted self-information maps\n","    \"\"\"\n","    n, c, h, w = prob.size()\n","    return -torch.mul(prob, torch.log2(prob + 1e-30)) / np.log2(c)\n","\n","\n","def XAI_EVAL(E_sgc, full_img_gt, model, rrp_info,target_category):\n","\n","    x_sgc = totensor(E_sgc)\n","    # x_sgc = x_sgc.cuda()\n","    with torch.no_grad():\n","        y_pred_sgc = model(x_sgc)\n","        y_pred_sgc = unpad_resize(y_pred_sgc,rrp_info)\n","\n","    mask_tensor_sgc = y_pred_sgc[0,...]\n","    mask_sgc = y_pred_sgc[0,...].cpu().numpy().transpose(1,2,0)\n","    target_mask_f = np.float32(mask_sgc[:,:,target_category]) * full_img_gt\n","    target_Confidence_score = target_mask_f[np.nonzero(target_mask_f)]\n","    target_Confidence_score_sgc = np.mean(target_Confidence_score)\n","\n","    logist_softmax_entropy_sgc = prob_2_entropy(y_pred_sgc)\n","    target_entropy_mask_sgc = logist_softmax_entropy_sgc[0, :, :, :].detach().cpu().numpy().transpose(1,2,0)\n","    target_entropy_mask_class_sgc = target_entropy_mask_sgc[:,:,target_category] * full_img_gt\n","    target_entropy_mask_class_sgc_sc = (np.mean(target_entropy_mask_class_sgc))\n","    return target_Confidence_score_sgc, target_entropy_mask_class_sgc_sc\n","\n","\n","images_dir = r'./dataset/images'\n","gt_dir = r'./dataset/gt'\n","\n","\n","\n","\n","\n","ticks = np.linspace(0, 1, 6, endpoint=True)\n","classes_cmap, scale_fig, fonts, scatter_size = defaultScales()\n","\n","\n","\n","\n","decoder_idx = 2\n","thres = 0.4\n","n_xai = 6\n","n_imgs = 1\n","target_layers =  [model.decoder.blocks[decoder_idx - 1]]\n","target_category = 0\n","XAI_method_t3 = [\"grad_cam\", \"grad_cam_pp\", \"x_grad_cam\",\"score_cam\", \"eigen_cam\"]\n","\n","image_weight = 0.006\n","\n","directory_images = os.fsencode(images_dir)\n","directory_gt = os.fsencode(gt_dir)\n","\n","\n","number_testing_images = 0\n","\n","Model_Seg_Score_t3 = np.zeros((n_imgs))\n","Model_Seg_Entropy_t3 = np.zeros((n_imgs))\n","\n","Seg_Score_M1_t3 = np.zeros((n_xai, n_imgs))\n","Seg_Entropy_M1_t3 = np.zeros((n_xai, n_imgs))\n","\n","Seg_Score_M2_t3 = np.zeros((n_xai, n_imgs))\n","Seg_Entropy_M2_t3 = np.zeros((n_xai, n_imgs))\n","\n","Seg_Score_M3_t3 = np.zeros((n_xai, n_imgs))\n","Seg_Entropy_M3_t3 = np.zeros((n_xai, n_imgs))\n","\n","counter = 0\n","\n","\n","for (file_img, file_gt) in zip(os.listdir(directory_images), os.listdir(directory_gt) ):\n","\n","    print(file_img, file_gt)\n","    if file_img == file_gt:\n","        number_testing_images = number_testing_images + 1\n","        print(\"Testing Image: \", number_testing_images)\n","        filename = os.fsdecode(file_img)\n","        filename_gt = os.fsdecode(file_gt)\n","        if filename.endswith(\".png\") and filename_gt.endswith(\".png\"):\n","            raster_file = rio.open(f'{images_dir}/{filename}')\n","            full_img = raster_file.read().transpose(1,2,0)\n","            full_img,rrp_info = ratio_resize_pad(full_img, ratio = None)\n","            full_img_rgba = full_img\n","            print(\"Input Image Shape: \", full_img.shape)\n","            if full_img.shape[-1]==4:full_img = cv2.cvtColor(full_img,cv2.COLOR_RGBA2RGB) #WHU images are RGBA\n","            # read gt mask\n","            raster_file_gt = rio.open(f'{gt_dir}/{filename_gt}')\n","            full_img_gt = raster_file_gt.read().transpose(1,2,0)\n","            full_img_gt,rrp_info_gt = ratio_resize_pad(full_img_gt, ratio = None)\n","            full_img_gt = np.float32(full_img_gt) / np.max(full_img_gt)\n","            print(\"GT mask shape: \", full_img_gt.shape)\n","\n","\n","            full_img = normalize(full_img)\n","            x = totensor(full_img)\n","            # x = x.cuda()\n","\n","            with torch.no_grad():\n","                y_pred = model(x)\n","                y_pred = unpad_resize(y_pred,rrp_info)\n","\n","\n","\n","            mask_tensor = y_pred[0,...]\n","            print('predicated tensor shape: ', mask_tensor.shape)\n","            mask = y_pred[0,...].cpu().numpy().transpose(1,2,0)\n","\n","            target_mask_float = np.float32(mask[:,:,target_category]) * full_img_gt\n","            target_Confidence_score = target_mask_float[np.nonzero(target_mask_float)]\n","            target_Confidence_score = np.mean(target_Confidence_score)\n","            print(\"Model Target Confidence Score: \", target_Confidence_score)\n","\n","            logist_softmax_entropy = prob_2_entropy(y_pred)\n","            target_entropy_mask = logist_softmax_entropy[0, :, :, :].detach().cpu().numpy().transpose(1,2,0)\n","            target_entropy_mask_class = target_entropy_mask[:,:,target_category] * full_img_gt\n","            model_entropy = np.mean(target_entropy_mask_class)\n","            print(\"Model Target Entropy Score: \", model_entropy )\n","\n","\n","            targets = [SemanticSegmentationTarget(target_category, target_mask_float)]\n","\n","            nan_condition = np.count_nonzero(target_mask_float)\n","            if nan_condition != 0:\n","\n","                print('running seg-grad-cam...')\n","                with GRADCAM_Extensions(extension = XAI_method_t3[0], model=model, target_layers=target_layers, use_cuda=torch.cuda.is_available()) as cam:\n","                    grayscale_cam_EX = cam(input_tensor=x, targets=targets)[0, :]\n","                    [Exmap_sgc_t3, heatmap_sgc_rgba] = show_cam_image_whu(full_img_rgba, grayscale_cam_EX, image_weight)\n","\n","                    # Evaluation\n","                    im_bw_sgc = cv2.threshold(grayscale_cam_EX, thres, 1, cv2.THRESH_BINARY)[1]\n","                    # M1: Background Only\n","                    E_sgc_M1 = full_img * np.logical_not(im_bw_sgc)[..., None]\n","                    [confidence_sgc_M1_t3, entropy_sgc_M1_t3] = XAI_EVAL(E_sgc_M1, full_img_gt, model, rrp_info,target_category)\n","                    print(\"M1 -- SGC Target Confidence Score: \", confidence_sgc_M1_t3,\" Entropy Score: \", entropy_sgc_M1_t3)\n","\n","                    # M2: Highlighted Only\n","                    E_sgc_M2 = full_img * im_bw_sgc[..., None]\n","                    [confidence_sgc_M2_t3, entropy_sgc_M2_t3] = XAI_EVAL(E_sgc_M2, full_img_gt, model, rrp_info,target_category)\n","                    print(\"M2 -- SGC Target Confidence Score: \", confidence_sgc_M2_t3,\" Entropy Score: \", entropy_sgc_M2_t3)\n","\n","                    # M3 : Highlighted + GT\n","                    union_gt_sgc = np.ma.mask_or(full_img_gt,im_bw_sgc)\n","                    E_sgc_M3_t3 = full_img * union_gt_sgc[..., None]\n","\n","                    [confidence_sgc_M3_t3, entropy_sgc_M3_t3] = XAI_EVAL(E_sgc_M3_t3, full_img_gt, model, rrp_info,target_category)\n","                    print(\"M3 -- SGC Target Confidence Score: \", confidence_sgc_M3_t3,\" Entropy Score: \", entropy_sgc_M3_t3)\n","                #______________________________\n","                print('running GradCam++ ...')\n","                with GRADCAM_Extensions(extension = XAI_method_t3[1], model=model, target_layers=target_layers, use_cuda=torch.cuda.is_available()) as campp:\n","                    grayscale_cam_EX_Plusplus = campp(input_tensor=x, targets=targets)[0, :]\n","                    [Exmap_sgcpp_t3, heatmap_sgcpp_rgba] = show_cam_image_whu(full_img_rgba, grayscale_cam_EX_Plusplus, image_weight)\n","\n","                \t\t# Evaluation\n","                    im_bw_sgcpp = cv2.threshold(grayscale_cam_EX_Plusplus, thres, 1, cv2.THRESH_BINARY)[1]\n","\n","                    # M1: Background Only\n","                    E_sgcpp_M1 = full_img * np.logical_not(im_bw_sgcpp)[..., None]\n","                    [confidence_sgcpp_M1_t3, entropy_sgcpp_M1_t3] = XAI_EVAL(E_sgcpp_M1, full_img_gt, model, rrp_info,target_category)\n","                    print(\"M1 -- SGC++ Target Confidence Score: \", confidence_sgcpp_M1_t3,\" Entropy Score: \", entropy_sgcpp_M1_t3)\n","\n","\t\t\t\t\t\t\t\t\t\t# M2: Highlighted Only\n","                    E_sgcpp_M2 = full_img * im_bw_sgcpp[..., None]\n","                    [confidence_sgcpp_M2_t3, entropy_sgcpp_M2_t3] = XAI_EVAL(E_sgcpp_M2, full_img_gt, model, rrp_info,target_category)\n","                    print(\"M2 -- SGC++ Target Confidence Score: \", confidence_sgcpp_M2_t3,\" Entropy Score: \", entropy_sgcpp_M2_t3)\n","\n","              \t\t\t# M3: Highlighted + GT\n","                    union_gt_sgcpp = np.ma.mask_or(full_img_gt,im_bw_sgcpp)\n","                    E_sgcpp_M3_t3 = full_img * union_gt_sgcpp[..., None]\n","                    [confidence_sgcpp_M3_t3, entropy_sgcpp_M3_t3] = XAI_EVAL(E_sgcpp_M3_t3, full_img_gt, model, rrp_info,target_category)\n","                    print(\"M3 -- SGC++ Target Confidence Score: \", confidence_sgcpp_M3_t3,\" Entropy Score: \", entropy_sgcpp_M3_t3)\n","\n","\t\t\t\t\t\t\t\t#___________________________\n","                print('running XGradCam...')\n","                with GRADCAM_Extensions(extension = XAI_method_t3[2], model=model, target_layers=target_layers, use_cuda=torch.cuda.is_available()) as xcam:\n","                    grayscale_cam_EX_X = xcam(input_tensor=x, targets=targets)[0, :]\n","                    [Exmap_Xsgc_t3, heatmap_Xsgc_rgba] = show_cam_image_whu(full_img_rgba, grayscale_cam_EX_X, image_weight)\n","\t\t\t\t\t\t\t\t\t\t# Evaluation\n","                    im_bw_Xsgc = cv2.threshold(grayscale_cam_EX_X, thres, 1, cv2.THRESH_BINARY)[1]\n","\n","                    # M1: Background Only\n","                    E_Xsgc_M1 = full_img * np.logical_not(im_bw_Xsgc)[..., None]\n","                    [confidence_Xsgc_M1_t3, entropy_Xsgc_M1_t3] = XAI_EVAL(E_Xsgc_M1, full_img_gt, model, rrp_info,target_category)\n","                    print(\"M1 -- XSGC Target Confidence Score: \", confidence_Xsgc_M1_t3,\" Entropy Score: \", entropy_Xsgc_M1_t3)\n","\n","\t\t\t\t\t\t\t\t\t\t# M2: Highlighted Only\n","                    E_Xsgc_M2 = full_img * im_bw_Xsgc[..., None]\n","                    [confidence_Xsgc_M2_t3, entropy_Xsgc_M2_t3] = XAI_EVAL(E_Xsgc_M2, full_img_gt, model, rrp_info,target_category)\n","                    print(\"M2 -- XSGC Target Confidence Score: \", confidence_Xsgc_M2_t3,\" Entropy Score: \", entropy_Xsgc_M2_t3)\n","\n","              \t\t\t# M3: Highlighted + GT\n","                    union_gt_Xsgc = np.ma.mask_or(full_img_gt,im_bw_Xsgc)\n","                    E_Xsgc_M3_t3 = full_img * union_gt_Xsgc[..., None]\n","                    [confidence_Xsgc_M3_t3, entropy_Xsgc_M3_t3] = XAI_EVAL(E_Xsgc_M3_t3, full_img_gt, model, rrp_info,target_category)\n","                    print(\"M3 -- XSGC Target Confidence Score: \", confidence_Xsgc_M3_t3,\" Entropy Score: \", entropy_Xsgc_M3_t3)\n","\n","\t\t\t\t\t\t\t\t#___________________________\n","                print('running ScoreCam...')\n","                with GRADCAM_Extensions(extension = XAI_method_t3[3], model=model, target_layers=target_layers, use_cuda=torch.cuda.is_available()) as scorecam:\n","                    grayscale_cam_EX_Score = scorecam(input_tensor=x, targets=targets)[0, :]\n","                    [Exmap_scoresgc_t3, heatmap_scoresgc_rgba] = show_cam_image_whu(full_img_rgba, grayscale_cam_EX_Score, image_weight)\n","\n","                    # Evaluation\n","                    im_bw_ssgc = cv2.threshold(grayscale_cam_EX_Score, thres, 1, cv2.THRESH_BINARY)[1]\n","\n","                    # M1: Background Only\n","                    E_ssgc_M1 = full_img * np.logical_not(im_bw_ssgc)[..., None]\n","                    [confidence_ssgc_M1_t3, entropy_ssgc_M1_t3] = XAI_EVAL(E_ssgc_M1, full_img_gt, model, rrp_info,target_category)\n","                    print(\"M1 -- Score SGC Target Confidence Score: \", confidence_ssgc_M1_t3,\" Entropy Score: \", entropy_ssgc_M1_t3)\n","\n","\t\t\t\t\t\t\t\t\t\t# M2: Highlighted Only\n","                    E_ssgc_M2 = full_img * im_bw_ssgc[..., None]\n","                    [confidence_ssgc_M2_t3, entropy_ssgc_M2_t3] = XAI_EVAL(E_ssgc_M2, full_img_gt, model, rrp_info,target_category)\n","                    print(\"M2 -- Score SGC Target Confidence Score: \", confidence_ssgc_M2_t3,\" Entropy Score: \", entropy_ssgc_M2_t3)\n","\n","              \t\t\t# M3: Highlighted + GT\n","                    union_gt_ssgc = np.ma.mask_or(full_img_gt,im_bw_ssgc)\n","                    E_ssgc_M3_t3 = full_img * union_gt_ssgc[..., None]\n","                    [confidence_ssgc_M3_t3, entropy_ssgc_M3_t3] = XAI_EVAL(E_ssgc_M3_t3, full_img_gt, model, rrp_info,target_category)\n","                    print(\"M3 -- Score SGC Target Confidence Score: \", confidence_ssgc_M3_t3,\" Entropy Score: \", entropy_ssgc_M3_t3)\n","\n","\t\t\t\t\t\t\t\t#__________________________\n","                print('running EigenCam...')\n","                with GRADCAM_Extensions(extension = XAI_method_t3[4], model=model, target_layers=target_layers, use_cuda=torch.cuda.is_available()) as ecam:\n","                    grayscale_cam_EX_eigen = ecam(input_tensor=x, targets=targets)[0, :]\n","                    [Exmap_eigensgc_t3, heatmap_eigensgc_rgba] = show_cam_image_whu(full_img_rgba, grayscale_cam_EX_eigen, image_weight)\n","\n","\t\t\t\t\t\t\t\t\t\t# Evaluation\n","                    im_bw_esgc = cv2.threshold(grayscale_cam_EX_eigen, thres, 1, cv2.THRESH_BINARY)[1]\n","\n","                    # M1: Background Only\n","                    E_esgc_M1 = full_img * np.logical_not(im_bw_esgc)[..., None]\n","                    [confidence_esgc_M1_t3, entropy_esgc_M1_t3] = XAI_EVAL(E_esgc_M1, full_img_gt, model, rrp_info,target_category)\n","                    print(\"M1 -- Eigen SGC Target Confidence Score: \", confidence_esgc_M1_t3,\" Entropy Score: \", entropy_esgc_M1_t3)\n","\n","\t\t\t\t\t\t\t\t\t\t# M2: Highlighted Only\n","                    E_esgc_M2 = full_img * im_bw_esgc[..., None]\n","                    [confidence_esgc_M2_t3, entropy_esgc_M2_t3] = XAI_EVAL(E_esgc_M2, full_img_gt, model, rrp_info,target_category)\n","                    print(\"M2 -- Eigen SGC Target Confidence Score: \", confidence_esgc_M2_t3,\" Entropy Score: \", entropy_esgc_M2_t3)\n","\n","              \t\t\t# M3: Highlighted + GT\n","                    union_gt_esgc = np.ma.mask_or(full_img_gt,im_bw_esgc)\n","                    E_esgc_M3_t3 = full_img * union_gt_esgc[..., None]\n","                    [confidence_esgc_M3_t3, entropy_esgc_M3_t3] = XAI_EVAL(E_esgc_M3_t3, full_img_gt, model, rrp_info,target_category)\n","                    print(\"M3 -- Eigen SGC Target Confidence Score: \", confidence_esgc_M3_t3,\" Entropy Score: \", entropy_esgc_M3_t3)\n","\n","                Model_Seg_Score_t3[counter] = target_Confidence_score\n","                Model_Seg_Entropy_t3[counter] = model_entropy\n","\n","                Seg_Score_M1_t3[0,counter] = confidence_sgc_M1_t3\n","                Seg_Score_M1_t3[1,counter] = confidence_sgcpp_M1_t3\n","                Seg_Score_M1_t3[2,counter] = confidence_Xsgc_M1_t3\n","                Seg_Score_M1_t3[3,counter] = confidence_ssgc_M1_t3\n","                Seg_Score_M1_t3[4,counter] = confidence_esgc_M1_t3\n","\n","                Seg_Entropy_M1_t3[0,counter] = entropy_sgc_M1_t3\n","                Seg_Entropy_M1_t3[1,counter] = entropy_sgcpp_M1_t3\n","                Seg_Entropy_M1_t3[2,counter] = entropy_Xsgc_M1_t3\n","                Seg_Entropy_M1_t3[3,counter] = entropy_ssgc_M1_t3\n","                Seg_Entropy_M1_t3[4,counter] = entropy_esgc_M1_t3\n","\n","\n","                Seg_Score_M2_t3[0,counter] = confidence_sgc_M2_t3\n","                Seg_Score_M2_t3[1,counter] = confidence_sgcpp_M2_t3\n","                Seg_Score_M2_t3[2,counter] = confidence_Xsgc_M2_t3\n","                Seg_Score_M2_t3[3,counter] = confidence_ssgc_M2_t3\n","                Seg_Score_M2_t3[4,counter] = confidence_esgc_M2_t3\n","\n","                Seg_Entropy_M2_t3[0,counter] = entropy_sgc_M2_t3\n","                Seg_Entropy_M2_t3[1,counter] = entropy_sgcpp_M2_t3\n","                Seg_Entropy_M2_t3[2,counter] = entropy_Xsgc_M2_t3\n","                Seg_Entropy_M2_t3[3,counter] = entropy_ssgc_M2_t3\n","                Seg_Entropy_M2_t3[4,counter] = entropy_esgc_M2_t3\n","\n","                Seg_Score_M3_t3[0,counter] = confidence_sgc_M3_t3\n","                Seg_Score_M3_t3[1,counter] = confidence_sgcpp_M3_t3\n","                Seg_Score_M3_t3[2,counter] = confidence_Xsgc_M3_t3\n","                Seg_Score_M3_t3[3,counter] = confidence_ssgc_M3_t3\n","                Seg_Score_M3_t3[4,counter] = confidence_esgc_M3_t3\n","\n","                Seg_Entropy_M3_t3[0,counter] = entropy_sgc_M3_t3\n","                Seg_Entropy_M3_t3[1,counter] = entropy_sgcpp_M3_t3\n","                Seg_Entropy_M3_t3[2,counter] = entropy_Xsgc_M3_t3\n","                Seg_Entropy_M3_t3[3,counter] = entropy_ssgc_M3_t3\n","                Seg_Entropy_M3_t3[4,counter] = entropy_esgc_M3_t3\n","\n","\n","                counter = counter + 1\n","\n","                if(number_testing_images == 2):\n","                    break\n","\n","\n","\n","                fig = plt.figure(figsize=(8 * scale_fig, 8 * scale_fig))\n","\n","                plt.subplot(3,3,1)\n","                plt.axis('off')\n","                plt.imshow(full_img_rgba)\n","                plt.title('Input image', fontsize=fonts)\n","\n","                plt.subplot(3,3,2)\n","                plt.axis('off')\n","                plt.imshow(full_img_gt, cmap='gray', vmin=0, vmax=1)\n","                plt.title('Ground Truth (GT) Mask', fontsize=fonts)\n","\n","                plt.subplot(3,3,3)\n","                plt.axis('off')\n","                plt.imshow(target_mask_float,  cmap='gray', vmin=0, vmax=1)\n","                plt.title('Predicted Mask', fontsize=fonts)\n","\n","                plt.subplot(3,3,4)\n","                plt.axis('off')\n","                plt.imshow(Exmap_sgc_t3)\n","                plt.title('Seg-Grad-CAM', fontsize=fonts)\n","\n","                plt.subplot(3,3,5)\n","                plt.axis('off')\n","                plt.imshow(Exmap_sgcpp_t3)\n","                plt.title('Seg-Grad-CAM ++', fontsize=fonts)\n","\n","                plt.subplot(3,3,6)\n","                plt.axis('off')\n","                plt.imshow(Exmap_Xsgc_t3)\n","                plt.title('Seg-XGrad-CAM', fontsize=fonts)\n","\n","                plt.subplot(3,3,7)\n","                plt.axis('off')\n","                plt.imshow(Exmap_scoresgc_t3)\n","                plt.title('Seg-Score-CAM', fontsize=fonts)\n","\n","                plt.subplot(3,3,8)\n","                plt.axis('off')\n","                plt.imshow(Exmap_eigensgc_t3)\n","                plt.title('Seg-Eigen-CAM', fontsize=fonts)\n","\n","                fig.savefig('./results/SegGradCam_Extensions_t3_{}_{}.png'.format(filename,decoder_idx), bbox_inches='tight', pad_inches=0)\n","\n","\n","                fig = plt.figure(figsize=(8 * scale_fig, 8 * scale_fig))\n","\n","                plt.subplot(3,3,1)\n","                plt.axis('off')\n","                plt.imshow(full_img_rgba)\n","                plt.title('Input image', fontsize=fonts)\n","\n","                plt.subplot(3,3,2)\n","                plt.axis('off')\n","                plt.imshow(full_img_gt, cmap='gray', vmin=0, vmax=1)\n","                plt.title('Ground Truth (GT) Mask', fontsize=fonts)\n","\n","                plt.subplot(3,3,3)\n","                plt.axis('off')\n","                plt.imshow(target_mask_float,  cmap='gray', vmin=0, vmax=1)\n","                plt.title('Predicted Mask', fontsize=fonts)\n","\n","                plt.subplot(3,3,4)\n","                plt.axis('off')\n","                plt.imshow(E_Xsgc_M3_t3)\n","                plt.title('Seg-Grad-CAM', fontsize=fonts)\n","\n","                plt.subplot(3,3,5)\n","                plt.axis('off')\n","                plt.imshow(E_sgcpp_M3_t3)\n","                plt.title('Seg-Grad-CAM ++', fontsize=fonts)\n","\n","                plt.subplot(3,3,6)\n","                plt.axis('off')\n","                plt.imshow(E_Xsgc_M3_t3)\n","                plt.title('Seg-XGrad-CAM', fontsize=fonts)\n","\n","                plt.subplot(3,3,7)\n","                plt.axis('off')\n","                plt.imshow(E_ssgc_M3_t3)\n","                plt.title('Seg-Score-CAM', fontsize=fonts)\n","\n","                plt.subplot(3,3,8)\n","                plt.axis('off')\n","                plt.imshow(E_esgc_M3_t3)\n","                plt.title('Seg-Eigen-CAM', fontsize=fonts)\n","\n","                fig.savefig('./results/SegGradCam_Modified_M3_t3_{}_{}.png'.format(filename,decoder_idx), bbox_inches='tight', pad_inches=0)\n"]},{"cell_type":"markdown","metadata":{"id":"0J7jJolDeb0C"},"source":["## Repeating Task 1"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FcCchu49f2nz","executionInfo":{"status":"ok","timestamp":1747933686509,"user_tz":-180,"elapsed":421239,"user":{"displayName":"Mohamad Sandakli","userId":"16501406067787434423"}},"outputId":"527c5c84-5da3-4be6-92d5-797950a7df2b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Running grad_cam ...\n","Running grad_cam_pp ...\n","Running x_grad_cam ...\n","Running score_cam ...\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 16/16 [05:16<00:00, 19.78s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Running eigen_cam ...\n"]}],"source":["results_t3 = {\n","    method: {\n","        th: {'M1': {}, 'M2': {}, 'M3': {}} for th in thresholds\n","    }\n","    for method in XAI_method_t3\n","}\n","\n","for method_name in XAI_method_t3:\n","    print(f'Running {method_name} ...')\n","\n","    with GRADCAM_Extensions(extension=method_name, model=model, target_layers=target_layers, use_cuda=torch.cuda.is_available()) as cam:\n","        grayscale_cam = cam(input_tensor=x, targets=targets)[0, :]\n","\n","        for thres in thresholds:\n","            im_bw = cv2.threshold(grayscale_cam, thres, 1, cv2.THRESH_BINARY)[1]\n","\n","            # M1: Background Only\n","            E_M1 = full_img * np.logical_not(im_bw)[..., None]\n","            confidence_M1, entropy_M1 = XAI_EVAL(E_M1, full_img_gt, model, rrp_info, target_category)\n","\n","            # M2: Highlighted Only\n","            E_M2 = full_img * im_bw[..., None]\n","            confidence_M2, entropy_M2 = XAI_EVAL(E_M2, full_img_gt, model, rrp_info, target_category)\n","\n","            # M3: Highlighted + GT\n","            union_mask = np.ma.mask_or(full_img_gt, im_bw)\n","            E_M3 = full_img * union_mask[..., None]\n","            confidence_M3, entropy_M3 = XAI_EVAL(E_M3, full_img_gt, model, rrp_info, target_category)\n","\n","            results_t3[method_name][thres]['M1'] = {'confidence': confidence_M1, 'entropy': entropy_M1}\n","            results_t3[method_name][thres]['M2'] = {'confidence': confidence_M2, 'entropy': entropy_M2}\n","            results_t3[method_name][thres]['M3'] = {'confidence': confidence_M3, 'entropy': entropy_M3}\n"]},{"cell_type":"markdown","metadata":{"id":"oCUZFvAT3_OC"},"source":["**Printing Results**"]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MhvlRxjjriA7","executionInfo":{"status":"ok","timestamp":1747933699111,"user_tz":-180,"elapsed":112,"user":{"displayName":"Mohamad Sandakli","userId":"16501406067787434423"}},"outputId":"2651037e-bb17-4f8f-d981-08c48a4e214e"},"outputs":[{"output_type":"stream","name":"stdout","text":["     Method  Threshold Metric  Confidence  Entropy\n","  eigen_cam        0.1     M1    0.030938 0.000691\n","  eigen_cam        0.1     M2    0.000021 0.000032\n","  eigen_cam        0.1     M3    0.168842 0.003327\n","  eigen_cam        0.2     M1    0.089913 0.002056\n","  eigen_cam        0.2     M2    0.000022 0.000033\n","  eigen_cam        0.2     M3    0.048698 0.001535\n","  eigen_cam        0.3     M1    0.188565 0.003024\n","  eigen_cam        0.3     M2    0.000323 0.000078\n","  eigen_cam        0.3     M3    0.022349 0.001169\n","  eigen_cam        0.4     M1    0.358685 0.003887\n","  eigen_cam        0.4     M2    0.173499 0.003378\n","  eigen_cam        0.4     M3    0.038197 0.001090\n","  eigen_cam        0.5     M1    0.466749 0.004320\n","  eigen_cam        0.5     M2    0.744334 0.003155\n","  eigen_cam        0.5     M3    0.021010 0.000897\n","   grad_cam        0.1     M1    0.011093 0.000573\n","   grad_cam        0.1     M2    0.040254 0.000425\n","   grad_cam        0.1     M3    0.027777 0.001038\n","   grad_cam        0.2     M1    0.028823 0.001430\n","   grad_cam        0.2     M2    0.274005 0.002862\n","   grad_cam        0.2     M3    0.015465 0.000646\n","   grad_cam        0.3     M1    0.116105 0.002785\n","   grad_cam        0.3     M2    0.554170 0.003307\n","   grad_cam        0.3     M3    0.016670 0.000709\n","   grad_cam        0.4     M1    0.103584 0.003314\n","   grad_cam        0.4     M2    0.750005 0.003955\n","   grad_cam        0.4     M3    0.017174 0.000706\n","   grad_cam        0.5     M1    0.286345 0.005483\n","   grad_cam        0.5     M2    0.842068 0.004748\n","   grad_cam        0.5     M3    0.018785 0.000714\n","grad_cam_pp        0.1     M1    0.000021 0.000032\n","grad_cam_pp        0.1     M2    0.229196 0.003104\n","grad_cam_pp        0.1     M3    0.238563 0.002787\n","grad_cam_pp        0.2     M1    0.000282 0.000066\n","grad_cam_pp        0.2     M2    0.022211 0.000553\n","grad_cam_pp        0.2     M3    0.050035 0.001285\n","grad_cam_pp        0.3     M1    0.003197 0.000232\n","grad_cam_pp        0.3     M2    0.214886 0.003600\n","grad_cam_pp        0.3     M3    0.017148 0.000625\n","grad_cam_pp        0.4     M1    0.058054 0.001856\n","grad_cam_pp        0.4     M2    0.673922 0.002980\n","grad_cam_pp        0.4     M3    0.015107 0.000584\n","grad_cam_pp        0.5     M1    0.206682 0.004750\n","grad_cam_pp        0.5     M2    0.844953 0.003929\n","grad_cam_pp        0.5     M3    0.015407 0.000705\n","  score_cam        0.1     M1    0.166425 0.002837\n","  score_cam        0.1     M2    0.737812 0.001414\n","  score_cam        0.1     M3    0.039112 0.000827\n","  score_cam        0.2     M1    0.335110 0.003513\n","  score_cam        0.2     M2    0.974102 0.001188\n","  score_cam        0.2     M3    0.020755 0.000810\n","  score_cam        0.3     M1    0.410601 0.003973\n","  score_cam        0.3     M2    0.993267 0.000493\n","  score_cam        0.3     M3    0.017636 0.000698\n","  score_cam        0.4     M1    0.489882 0.004771\n","  score_cam        0.4     M2    0.984366 0.000901\n","  score_cam        0.4     M3    0.019994 0.000754\n","  score_cam        0.5     M1    0.503589 0.004677\n","  score_cam        0.5     M2    0.980316 0.001014\n","  score_cam        0.5     M3    0.022627 0.000782\n"," x_grad_cam        0.1     M1    0.000850 0.000195\n"," x_grad_cam        0.1     M2    0.029617 0.000789\n"," x_grad_cam        0.1     M3    0.053106 0.001500\n"," x_grad_cam        0.2     M1    0.015484 0.000303\n"," x_grad_cam        0.2     M2    0.045909 0.000904\n"," x_grad_cam        0.2     M3    0.015666 0.000609\n"," x_grad_cam        0.3     M1    0.025713 0.001057\n"," x_grad_cam        0.3     M2    0.463582 0.003598\n"," x_grad_cam        0.3     M3    0.016837 0.000662\n"," x_grad_cam        0.4     M1    0.078741 0.002077\n"," x_grad_cam        0.4     M2    0.748008 0.003762\n"," x_grad_cam        0.4     M3    0.015066 0.000714\n"," x_grad_cam        0.5     M1    0.220581 0.005438\n"," x_grad_cam        0.5     M2    0.872690 0.004466\n"," x_grad_cam        0.5     M3    0.019001 0.000707\n"]}],"source":["rows = []\n","for method in results_t3:\n","    for th in results_t3[method]:\n","        for metric in results_t3[method][th]:\n","            row = {\n","                'Method': method,\n","                'Threshold': th,\n","                'Metric': metric,\n","                'Confidence': results_t3[method][th][metric]['confidence'],\n","                'Entropy': results_t3[method][th][metric]['entropy']\n","            }\n","            rows.append(row)\n","\n","df_results_t3 = pd.DataFrame(rows)\n","\n","df_results_t3 = df_results_t3.sort_values(by=['Method', 'Threshold', 'Metric'])\n","\n","print(df_results_t3.to_string(index=False))\n"]},{"cell_type":"markdown","metadata":{"id":"z_ZFLjRb4GJJ"},"source":["**Best XAI Method for each Eval Metric and threshold**"]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"maensdRftkCF","executionInfo":{"status":"ok","timestamp":1747933704631,"user_tz":-180,"elapsed":96,"user":{"displayName":"Mohamad Sandakli","userId":"16501406067787434423"}},"outputId":"58497ee8-efb3-4175-93b8-3cf5cf57afc4"},"outputs":[{"output_type":"stream","name":"stdout","text":["    Threshold Metric       Method  Confidence   Entropy\n","0         0.1     M1  grad_cam_pp    0.000021  0.000032\n","1         0.2     M1  grad_cam_pp    0.000282  0.000066\n","2         0.3     M1  grad_cam_pp    0.003197  0.000232\n","3         0.4     M1  grad_cam_pp    0.058054  0.001856\n","4         0.5     M1  grad_cam_pp    0.206682  0.004750\n","5         0.1     M2    score_cam    0.737812  0.001414\n","6         0.2     M2    score_cam    0.974102  0.001188\n","7         0.3     M2    score_cam    0.993267  0.000493\n","8         0.4     M2    score_cam    0.984366  0.000901\n","9         0.5     M2    score_cam    0.980316  0.001014\n","10        0.1     M3    score_cam    0.039112  0.000827\n","11        0.2     M3   x_grad_cam    0.015666  0.000609\n","12        0.3     M3  grad_cam_pp    0.017148  0.000625\n","13        0.4     M3  grad_cam_pp    0.015107  0.000584\n","14        0.5     M3  grad_cam_pp    0.015407  0.000705\n"]}],"source":["best_methods_t3 = []\n","\n","for (thresh, metric), group in df_results_t3.groupby(['Threshold', 'Metric']):\n","    if metric == 'M1':\n","        # Lowest confidence\n","        best_row = group.loc[group['Confidence'].idxmin()]\n","    elif metric == 'M2':\n","        # Highest confidence\n","        best_row = group.loc[group['Confidence'].idxmax()]\n","    elif metric == 'M3':\n","        # Lowest entropy\n","        best_row = group.loc[group['Entropy'].idxmin()]\n","    else:\n","        continue\n","\n","    best_methods_t3.append(best_row)\n","\n","best_methods_df_t3 = pd.DataFrame(best_methods_t3)\n","\n","best_methods_df_t3 = best_methods_df_t3.sort_values(['Metric', 'Threshold']).reset_index(drop=True)\n","print(best_methods_df_t3[['Threshold', 'Metric', 'Method', 'Confidence', 'Entropy']])\n"]},{"cell_type":"markdown","metadata":{"id":"B509Ltp88-bI"},"source":["**Overall Best XAI Method**"]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9ZMN5t0bl37N","executionInfo":{"status":"ok","timestamp":1747933708894,"user_tz":-180,"elapsed":66,"user":{"displayName":"Mohamad Sandakli","userId":"16501406067787434423"}},"outputId":"005de1ed-c48a-4b67-9b96-d0b479ada523"},"outputs":[{"output_type":"stream","name":"stdout","text":["Method Selection Frequency:\n","Method\n","grad_cam_pp    8\n","score_cam      6\n","x_grad_cam     1\n","Name: count, dtype: int64\n","\n","Best overall XAI method: grad_cam_pp\n"]}],"source":["method_score_t3 = best_methods_df_t3['Method'].value_counts()\n","\n","print(\"Method Selection Frequency:\")\n","print(method_score_t3)\n","\n","best_overall_method_t3 = method_score_t3.idxmax()\n","print(f\"\\nBest overall XAI method: {best_overall_method_t3}\")\n"]},{"cell_type":"markdown","metadata":{"id":"cACPSfakGem1"},"source":["**Best Threshold Value(s)**"]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kGqXaMuwGpwR","executionInfo":{"status":"ok","timestamp":1747933711477,"user_tz":-180,"elapsed":86,"user":{"displayName":"Mohamad Sandakli","userId":"16501406067787434423"}},"outputId":"7de556c4-e452-4f3d-b7ff-7a145d9e1e31"},"outputs":[{"output_type":"stream","name":"stdout","text":["Method: eigen_cam\n"," Best threshold: 0.5\n","\n","Method: grad_cam\n"," Best threshold: 0.4\n","\n","Method: grad_cam_pp\n"," Best threshold: 0.4\n","\n","Method: score_cam\n"," Best threshold: 0.3\n","\n","Method: x_grad_cam\n"," Best threshold: 0.4\n","\n"]}],"source":["def normalize(arr):\n","    return (arr - np.min(arr)) / (np.max(arr) - np.min(arr))\n","\n","best_thresholds = {}\n","\n","for method in df_results_t3['Method'].unique():\n","    df_m = df_results_t3[df_results_t3['Method'] == method]\n","\n","    conf_pivot = df_m.pivot(index='Threshold', columns='Metric', values='Confidence')\n","    entropy_pivot = df_m.pivot(index='Threshold', columns='Metric', values='Entropy')\n","\n","    m1_conf_norm = normalize(conf_pivot['M1'].values)\n","    m2_conf_norm = normalize(conf_pivot['M2'].values)\n","    m3_entropy_norm = normalize(entropy_pivot['M3'].values)\n","\n","    # Combined score: low M1 conf, high M2 conf, low M3 entropy\n","    combined_score = m1_conf_norm + (1 - m2_conf_norm) + m3_entropy_norm\n","\n","    best_idx = np.argmin(combined_score)\n","    best_thresh = conf_pivot.index[best_idx]\n","\n","    best_thresholds[method] = {\n","        'best_threshold': best_thresh,\n","        'combined_scores': dict(zip(conf_pivot.index, combined_score))\n","    }\n","\n","for method, info in best_thresholds.items():\n","    print(f\"Method: {method}\")\n","    print(f\" Best threshold: {info['best_threshold']}\")\n","    print()\n"]},{"cell_type":"markdown","metadata":{"id":"QV2B01XI4R3N"},"source":["**Comparing results of Task 1 and Task 3**"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"N5JtaYW_8g5f","executionInfo":{"status":"ok","timestamp":1747933715505,"user_tz":-180,"elapsed":68,"user":{"displayName":"Mohamad Sandakli","userId":"16501406067787434423"}}},"outputs":[],"source":["def flatten_results(results):\n","    rows = []\n","    for method, th_dict in results.items():\n","        for th, metrics in th_dict.items():\n","            for mi, values in metrics.items():\n","                rows.append({\n","                    'Method': method,\n","                    'Threshold': th,\n","                    'Metric': mi,\n","                    'Confidence': values['confidence'],\n","                    'Entropy': values['entropy']\n","                })\n","    return pd.DataFrame(rows)"]},{"cell_type":"code","execution_count":31,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xtzMYNIH8DiH","executionInfo":{"status":"ok","timestamp":1747933721079,"user_tz":-180,"elapsed":232,"user":{"displayName":"Mohamad Sandakli","userId":"16501406067787434423"}},"outputId":"91aec037-b833-4513-f35b-595f04015ceb"},"outputs":[{"output_type":"stream","name":"stdout","text":["XAI Method: grad_cam\n","  M1 (Background Only - ↓ Conf): 5/5 improved\n","  M2 (Highlighted Only - ↑ Conf): 3/5 improved\n","  M3 (Highlight+GT - ↓ Entropy): 5/5 improved\n","  --> Overall Improvement: Yes\n","--------------------------------------------------\n","XAI Method: grad_cam_pp\n","  M1 (Background Only - ↓ Conf): 5/5 improved\n","  M2 (Highlighted Only - ↑ Conf): 3/5 improved\n","  M3 (Highlight+GT - ↓ Entropy): 4/5 improved\n","  --> Overall Improvement: Yes\n","--------------------------------------------------\n","XAI Method: x_grad_cam\n","  M1 (Background Only - ↓ Conf): 5/5 improved\n","  M2 (Highlighted Only - ↑ Conf): 3/5 improved\n","  M3 (Highlight+GT - ↓ Entropy): 5/5 improved\n","  --> Overall Improvement: Yes\n","--------------------------------------------------\n","XAI Method: score_cam\n","  M1 (Background Only - ↓ Conf): 1/5 improved\n","  M2 (Highlighted Only - ↑ Conf): 4/5 improved\n","  M3 (Highlight+GT - ↓ Entropy): 5/5 improved\n","  --> Overall Improvement: Yes\n","--------------------------------------------------\n","XAI Method: eigen_cam\n","  M1 (Background Only - ↓ Conf): 5/5 improved\n","  M2 (Highlighted Only - ↑ Conf): 1/5 improved\n","  M3 (Highlight+GT - ↓ Entropy): 4/5 improved\n","  --> Overall Improvement: Yes\n","--------------------------------------------------\n"]}],"source":["df_t1 = flatten_results(results_t1)\n","df_t3 = flatten_results(results_t3)\n","\n","df_compare = df_t1.merge(df_t3, on=['Method', 'Threshold', 'Metric'], suffixes=('_t1', '_t3'))\n","\n","method_summary = {}\n","\n","for method in df_compare['Method'].unique():\n","    method_df = df_compare[df_compare['Method'] == method]\n","    improved_metrics = {'M1': 0, 'M2': 0, 'M3': 0}\n","    total_counts = {'M1': 0, 'M2': 0, 'M3': 0}\n","\n","    for _, row in method_df.iterrows():\n","        metric = row['Metric']\n","        total_counts[metric] += 1\n","\n","        if metric == 'M1':\n","            if row['Confidence_t3'] < row['Confidence_t1']:\n","                improved_metrics['M1'] += 1\n","        elif metric == 'M2':\n","            if row['Confidence_t3'] > row['Confidence_t1']:\n","                improved_metrics['M2'] += 1\n","        elif metric == 'M3':\n","            if row['Entropy_t3'] < row['Entropy_t1']:\n","                improved_metrics['M3'] += 1\n","\n","    method_summary[method] = {\n","        'M1_Improved': f\"{improved_metrics['M1']}/{total_counts['M1']}\",\n","        'M2_Improved': f\"{improved_metrics['M2']}/{total_counts['M2']}\",\n","        'M3_Improved': f\"{improved_metrics['M3']}/{total_counts['M3']}\",\n","    }\n","\n","for method, scores in method_summary.items():\n","    print(f\"XAI Method: {method}\")\n","    print(f\"  M1 (Background Only - ↓ Conf): {scores['M1_Improved']} improved\")\n","    print(f\"  M2 (Highlighted Only - ↑ Conf): {scores['M2_Improved']} improved\")\n","    print(f\"  M3 (Highlight+GT - ↓ Entropy): {scores['M3_Improved']} improved\")\n","    print(\"  --> Overall Improvement:\", \"Yes\" if any(int(x.split('/')[0]) > 0 for x in scores.values()) else \"No\")\n","    print(\"-\" * 50)\n"]},{"cell_type":"markdown","metadata":{"id":"WtMwT4chdUNH"},"source":["## Repeating Task 2"]},{"cell_type":"code","execution_count":32,"metadata":{"id":"KhTtqcsleWrl","executionInfo":{"status":"ok","timestamp":1747933730222,"user_tz":-180,"elapsed":23,"user":{"displayName":"Mohamad Sandakli","userId":"16501406067787434423"}}},"outputs":[],"source":["thres = 0.4"]},{"cell_type":"code","execution_count":33,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CGWzxnfidB5j","executionInfo":{"status":"ok","timestamp":1747934074640,"user_tz":-180,"elapsed":343238,"user":{"displayName":"Mohamad Sandakli","userId":"16501406067787434423"}},"outputId":"2217128e-4c0b-47dd-e10d-b2f0e86d9922"},"outputs":[{"output_type":"stream","name":"stdout","text":["running grad_cam ...\n","grad_cam Drop in Segmentation:  -0.28235038241062205\n","running grad_cam_pp ...\n","grad_cam_pp Drop in Segmentation:  -0.2809565167789135\n","running x_grad_cam ...\n","x_grad_cam Drop in Segmentation:  -0.28192515682477465\n","running score_cam ...\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 16/16 [05:08<00:00, 19.29s/it]\n"]},{"output_type":"stream","name":"stdout","text":["score_cam Drop in Segmentation:  -0.27227709757772023\n","running eigen_cam ...\n","eigen_cam Drop in Segmentation:  -0.23373582717876795\n"]}],"source":["drop_results_t3 = {method: {} for method in XAI_method_t3}\n","\n","for method_name in XAI_method_t3:\n","    print(f'running {method_name} ...')\n","\n","    with GRADCAM_Extensions(extension=method_name, model=model, target_layers=target_layers, use_cuda=torch.cuda.is_available()) as cam:\n","        grayscale_cam = cam(input_tensor=x, targets=targets)[0, :]\n","\n","        im_bw = cv2.threshold(grayscale_cam, thres, 1, cv2.THRESH_BINARY)[1]\n","\n","        # M3: Highlighted + GT\n","        union_mask = np.ma.mask_or(full_img_gt, im_bw)\n","        E_img = full_img * union_mask[..., None]\n","        drop_result_t3 = XAI_EVAL_Drop(E_img, full_img_gt, model, full_img, rrp_info, target_category)\n","\n","        print(f\"{method_name} Drop in Segmentation: \", drop_result_t3)\n","        drop_results_t3[method_name] = {\n","            'drop': drop_result_t3\n","        }\n"]},{"cell_type":"code","execution_count":34,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EbQcpzs9_Rku","executionInfo":{"status":"ok","timestamp":1747934078514,"user_tz":-180,"elapsed":58,"user":{"displayName":"Mohamad Sandakli","userId":"16501406067787434423"}},"outputId":"af12d857-fb8c-46c4-8477-9fd6ce67b7ea"},"outputs":[{"output_type":"stream","name":"stdout","text":["XAI Methods Ranked by Faithfulness to the model’s true reasoning:\n","1. grad_cam: Drop = -0.2824\n","2. x_grad_cam: Drop = -0.2819\n","3. grad_cam_pp: Drop = -0.2810\n","4. score_cam: Drop = -0.2723\n","5. eigen_cam: Drop = -0.2337\n"]}],"source":["ranked_methods_t3 = sorted(drop_results_t3.items(), key=lambda x: x[1]['drop'])\n","\n","print(\"XAI Methods Ranked by Faithfulness to the model’s true reasoning:\")\n","for i, (method, result) in enumerate(ranked_methods_t3, start=1):\n","    print(f\"{i}. {method}: Drop = {result['drop']:.4f}\")\n"]},{"cell_type":"code","execution_count":35,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GlSw0Fne_DiK","executionInfo":{"status":"ok","timestamp":1747934082883,"user_tz":-180,"elapsed":98,"user":{"displayName":"Mohamad Sandakli","userId":"16501406067787434423"}},"outputId":"850916af-5091-43b0-80f4-c6f358281f4c"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n"," Best XAI method (lowest drop in IoU): grad_cam with drop = -0.2824\n"]}],"source":["best_method_t3, best_result_t3 = ranked_methods_t3[0]\n","\n","print(f\"\\n Best XAI method (lowest drop in IoU): {best_method_t3} with drop = {best_result_t3['drop']:.4f}\")\n"]},{"cell_type":"markdown","metadata":{"id":"UM9jC-m34ZF-"},"source":["Comparing results of Task 2 and Task 3"]},{"cell_type":"code","execution_count":36,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lRY0Qk8Vo4on","executionInfo":{"status":"ok","timestamp":1747934217226,"user_tz":-180,"elapsed":121,"user":{"displayName":"Mohamad Sandakli","userId":"16501406067787434423"}},"outputId":"c2d58f47-0b1e-42ae-c85e-23d1583afd70"},"outputs":[{"output_type":"stream","name":"stdout","text":["IoU drop has increased (worsened) in Task 3 compared to Task 2.\n"]}],"source":["if best_result_t3['drop'] < best_result_t2['drop']:\n","    trend = \"decreased (improved)\"\n","elif best_result_t3['drop'] > best_result_t2['drop']:\n","    trend = \"increased (worsened)\"\n","else:\n","    trend = \"unchanged\"\n","\n","print(f\"IoU drop has {trend} in Task 3 compared to Task 2.\")\n"]},{"cell_type":"markdown","metadata":{"id":"QIqtWAe5HZp2"},"source":["---\n","Incorporating a second decoder can modify the model’s segmentation outputs and internal feature learning, thereby impacting the clarity, reliability, and stability of the XAI explanations."]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":0}